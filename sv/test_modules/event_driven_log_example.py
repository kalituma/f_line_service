try:
    import ray  # type: ignore
except ImportError:
    ray = None  # type: ignore

from typing import Optional, Dict, Any
from threading import Thread, Event as ThreadEvent
from datetime import datetime
import time

import os
from sv import PROJECT_ROOT_PATH
from sv.daemon.module.db_change_listener import DBChangeListener, DBChangeEvent, ChangeEventType
from sv.daemon.module.execution_engine import ExecutionEngine
from sv.daemon.module.work_manager import WorkManager
from sv.daemon.module.task_manager import TaskManager
from sv.utils.logger import setup_logger
from sv.backend.service.service_manager import get_service_manager
from sv.daemon.module.update_handler import send_video_status_update
from sv.daemon.module.http_request_client import post_request, HttpRequestError
from sv.daemon.server_state import ServerAnalysisStatus
from sv.daemon.daemon_state import STATUS_FAILED, STATUS_SUCCESS, STATUS_NOT_EXISTS

from sv.test_modules.test_tasks import split_primary_task_result
from sv.task.mock.connection_task import ConnectionTask
from sv.task.mock.video_extract_task import VideoFrameExtractionTask
from sv.task.mock.segmentation_task import VideoSegmentationTask
from sv.task.mock.location_simulation_task import LocationSimulationTask
from sv.task.mock.feature_matching_task import FeatureMatchingTask
from sv.task.mock.geojson_boundary_task import SegmentationGeoJsonTask

logger = setup_logger(__name__)


def initialize_logger() -> None:
    setup_common_logger(None)


service_manager = get_service_manager()
if not service_manager.is_initialized():
    logger.info("ServiceManager ì´ˆê¸°í™” ì¤‘...")
    service_manager.initialize_all_services()


class EventDrivenLogger:
    def __init__(
            self,
            poll_interval: float = 2.0,
            num_executors: int = 2,
            run_once: bool = False,
    ):
        self.video_request_url = "http://127.0.0.1:8086/wildfire-data-sender/api/wildfire/sender"
        self.analysis_update_url = "http://127.0.0.1:8086/wildfire-data-receiver/api/wildfire/video-status"
        self.result_sent_url = "http://127.0.0.1:8086/wildfire-data-receiver/api/wildfire/data"
        self.work_dir = os.path.join(PROJECT_ROOT_PATH, 'data', 'workspace')
        self.result_path = os.path.join(PROJECT_ROOT_PATH, 'data', 'vid', 'cy_all.geojson')

        self.work_manager = WorkManager()
        self.task_manager = TaskManager()

        self.task_manager.register_primary_task(ConnectionTask(api_url=self.video_request_url))
        self.task_manager.register_secondary_tasks(
            [VideoFrameExtractionTask(delay_seconds=5), VideoSegmentationTask(delay_seconds=5),
             LocationSimulationTask(delay_seconds=10), FeatureMatchingTask(delay_seconds=20),
             SegmentationGeoJsonTask(result_path=self.result_path, delay_seconds=5)])
        self.task_manager.set_data_splitter(split_primary_task_result)

        self.execution_engine = ExecutionEngine(base_work_dir=self.work_dir, update_url=self.analysis_update_url,
                                                num_executors=num_executors)

        self.db_listener = DBChangeListener(poll_interval)
        self.db_listener.on_change(self._handle_db_change)
        self.running = False
        self.run_once = run_once  # 1íšŒ ì‹¤í–‰ ëª¨ë“œ

        self.listener_thread: Optional[Thread] = None
        self.monitor_thread: Optional[Thread] = None

        self.poll_interval = poll_interval
        self.stop_event = ThreadEvent()

    ############################################## event processor ##############################################

    def _handle_db_change(self, event: DBChangeEvent) -> None:
        """
        DB ë³€ê²½ ì´ë²¤íŠ¸ ë‚´ë¶€ í•¸ë“¤ëŸ¬
        
        Args:
            event: DB ë³€ê²½ ì´ë²¤íŠ¸
        """
        logger.info(f"DB Change detected: {event}")

        # Work Queue INSERT ì´ë²¤íŠ¸ ì²˜ë¦¬
        if event.table_name == "work_queue" and event.event_type == ChangeEventType.PENDING_WORKS_DETECTED:
            logger.info(f"âœ“ New work detected: {event.data}")
            if self._on_work_created:
                try:
                    self._on_work_created()
                except Exception as e:
                    logger.error(f"Error in on_work_created callback: {str(e)}", exc_info=True)

    def check_changes(self) -> None:
        """
        Pending ìƒíƒœì˜ Work ê°œìˆ˜ í™•ì¸
        """
        try:
            self.db_listener.check_pending_works()
        except Exception as e:
            logger.error(f"Error checking pending works: {str(e)}")

    #########################################################################################################

    def _on_work_created(self) -> None:
        """Work ìƒì„± ê°ì§€ ì‹œ í•¸ë“¤ëŸ¬ (Event Processorì—ì„œ í˜¸ì¶œ)"""
        logger.info("Work creation event detected")
        self._process_pending_work()

    def _on_work_complete(self, work_id: str, result: Dict[str, Any]) -> None:
        """Work ì™„ë£Œ ì‹œ í•¸ë“¤ëŸ¬ (Event Processorì—ì„œ í˜¸ì¶œ)"""
        logger.info(f"Work {work_id} completed")

        # Work ìƒíƒœì— ë”°ë¼ ì²˜ë¦¬
        status = result.get('status')
        if status == 'failed':
            error_msg = result.get('error', 'Unknown error')
            logger.error(f"Work {work_id} failed: {error_msg}")
        else:
            self._update_video_status(work_id, result)
            self._send_analysis_results(work_id, result)

    def _update_video_status(self, work_id: str, result: Dict[str, Any]) -> None:
        """
        Work ìƒíƒœë¥¼ ì„œë²„ì— ì—…ë°ì´íŠ¸
        
        Args:
            work_id: Work ID
            result: Workì˜ ê²°ê³¼ ì •ë³´ (status í¬í•¨)
        """
        try:
            loop_context = result.get('loop_context')
            frfr_id = loop_context.get('frfr_id')
            analysis_id = loop_context.get('analysis_id')

            if not frfr_id or not analysis_id:
                logger.error(f"âŒ Missing frfr_id or analysis_id for work {work_id}")
                return

            video_updates = []
            videos_results = result.get('item_results')
            for video_result in videos_results:
                video = video_result.get('data_item')
                video_name = video.get('video_name')
                status = video_result.get('status')
                if status == STATUS_SUCCESS.to_str():
                    status_code = ServerAnalysisStatus.STAT_003.to_code()  # fline_extracted
                elif status == STATUS_FAILED.to_str():
                    status_code = ServerAnalysisStatus.STAT_004.to_code()  # fline_failed
                elif status == STATUS_NOT_EXISTS.to_str():
                    status_code = ServerAnalysisStatus.STAT_005.to_code()  # video_receive_failed
                else:
                    logger.error(f"âŒ Unknown status for video {video_name}: {status}")
                    continue
                video_updates.append({
                    "video_name": video_name,
                    "analysis_status": status_code
                })

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            logger.info(f"ğŸ“¤ Sending work status for work {work_id} to server...")

            send_video_status_update(
                update_url=self.analysis_update_url,
                frfr_id=frfr_id,
                analysis_id=analysis_id,
                video_updates=video_updates
            )

            logger.info(f"âœ… Work {work_id} status sent to server")

        except Exception as e:
            logger.error(f"âŒ Error updating work status for {work_id}: {str(e)}", exc_info=True)

    def _send_analysis_results(self, work_id: str, result: Dict[str, Any]) -> None:
        """
        ì„±ê³µí•œ ì‘ì—…ì˜ SegmentationGeoJsonTask ê²°ê³¼ë¥¼ ì„œë²„ë¡œ ì „ì†¡
        
        Args:
            work_id: Work ID
            result: Workì˜ ê²°ê³¼ ì •ë³´
        """
        try:
            loop_context = result.get('loop_context')
            frfr_id = loop_context.get('frfr_id')
            analysis_id = loop_context.get('analysis_id')
            
            if not frfr_id or not analysis_id:
                logger.error(f"âŒ Missing frfr_id or analysis_id for work {work_id}")
                return
            
            # item_resultsì—ì„œ ì„±ê³µí•œ í•­ëª©ë§Œ ì²˜ë¦¬
            videos_results = result.get('item_results', [])
            sent_count = 0
            
            for video_result in videos_results:
                status = video_result.get('status')
                
                # statusê°€ 'success'ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
                if status != STATUS_SUCCESS.to_str():
                    continue
                
                # tasksì—ì„œ SegmentationGeoJsonTask ê²°ê³¼ ì°¾ê¸°
                tasks = video_result.get('tasks', [])
                segmentation_task_result = None
                
                for task in tasks:
                    if task.get('task_name') == 'SegmentationGeoJsonTask':                        
                        segmentation_task_result = task.get('result')
                        break
                
                if not segmentation_task_result:
                    logger.warning("âš ï¸  No SegmentationGeoJsonTask result found for video")
                    continue
                
                # integrated_convex_hull ê°€ì ¸ì˜¤ê¸°
                integrated_convex_hull = segmentation_task_result.get('integrated_convex_hull')
                
                if not integrated_convex_hull:
                    logger.warning("âš ï¸  No integrated_convex_hull found in SegmentationGeoJsonTask result")
                    continue
                
                # GeoJSONì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                enriched_geojson = integrated_convex_hull.copy()
                
                # frfr_idë¥¼ frfr_info_idë¡œ ì¶”ê°€
                enriched_geojson['frfr_info_id'] = frfr_id
                
                # analysis_id ì¶”ê°€
                enriched_geojson['analysis_id'] = analysis_id
                
                # í˜„ì¬ ì‹œê°„ì„ yyyy-MM-dd HH:mm:ss í˜•íƒœë¡œ ì¶”ê°€
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                enriched_geojson['timestamp'] = current_time
                
                # ì„œë²„ë¡œ POST ì „ì†¡
                try:
                    logger.info(f"ğŸ“¤ Sending analysis result to {self.result_sent_url}...")
                    logger.debug(f"   frfr_info_id: {frfr_id}")
                    logger.debug(f"   analysis_id: {analysis_id}")
                    logger.debug(f"   timestamp: {current_time}")
                    
                    response = post_request(
                        url=self.result_sent_url,
                        json_data=enriched_geojson,
                        headers={'Content-Type': 'application/json'},
                        timeout=30
                    )
                    
                    logger.info("âœ… Analysis result sent successfully")
                    logger.debug(f"   Response: {response.text}")
                    sent_count += 1
                        
                except HttpRequestError as e:
                    logger.error(f"âŒ HTTP request error sending analysis result: {str(e)}")
                except Exception as e:
                    logger.error(f"âŒ Error sending analysis result: {str(e)}", exc_info=True)
            
            if sent_count > 0:
                logger.info(f"âœ… Sent {sent_count} analysis result(s) for work {work_id}")
            else:
                logger.info(f"â„¹ï¸  No analysis results to send for work {work_id}")
                
        except Exception as e:
            logger.error(f"âŒ Error in _send_analysis_results for {work_id}: {str(e)}", exc_info=True)

    def _process_pending_work(self) -> None:
        """
        ëŒ€ê¸° ì¤‘ì¸ Work ì²˜ë¦¬ (ë¹„ë™ê¸° ì½œë°± ë°©ì‹)
        
        Flow:
        1. ë‹¤ìŒ PENDING Work ê°€ì ¸ì˜¤ê¸°
        2. Task ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹)
        3. ì™„ë£Œ ì‹œ _on_work_complete ì½œë°± í˜¸ì¶œ
        """
        work_info = self.work_manager.get_next_pending_work()

        if not work_info:
            logger.debug("No pending works")
            return

        work_id = work_info['work_id']
        frfr_id = work_info['frfr_id']

        # Task ì‹¤í–‰ ì¤€ë¹„ í™•ì¸
        if not self.task_manager.are_tasks_ready():
            logger.error("âŒ Tasks not ready for execution")
            return

        # Task ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹ - ì½œë°± ë°©ì‹)
        self.execution_engine.execute_work(
            work_info=work_info,
            primary_task=self.task_manager.primary_task,
            secondary_tasks=self.task_manager.secondary_tasks,
            data_splitter=self.task_manager.data_splitter,
            on_job_complete=self._on_work_complete  # ì½œë°± í•¨ìˆ˜
        )

        logger.info(f"âœ“ Work {work_id} (frfr_id={frfr_id}) submitted")

    def _listener_thread_func(self):
        """DB ë³€ê²½ ê°ì§€ ë¦¬ìŠ¤ë„ˆ ìŠ¤ë ˆë“œ"""
        if self.run_once:
            logger.info("ğŸ§ DB Change Listener started (run_once mode)")
        else:
            logger.info("ğŸ§ DB Change Listener started (continuous mode)")

        while not self.stop_event.is_set():
            try:
                # Work Queue í…Œì´ë¸” ê°ì‹œ (PENDING ìƒíƒœ)
                self.check_changes()

                # run_once ëª¨ë“œë©´ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ì¢…ë£Œ
                if self.run_once:
                    logger.info("âœ“ Run once mode: completed one check cycle")
                    break

                time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"Error in listener thread: {str(e)}", exc_info=True)

                # run_once ëª¨ë“œì—ì„œë„ ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ
                if self.run_once:
                    break

                time.sleep(self.poll_interval)

        logger.info("ğŸ§ DB Change Listener stopped")

    def _ray_monitor_thread_func(self) -> None:
        """Ray Work ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œ (ExecutionEngineì˜ pending jobs ëª¨ë‹ˆí„°ë§)"""
        logger.info("âš¡ Ray Work Monitor started")

        while not self.stop_event.is_set():
            try:
                if not self.execution_engine:
                    time.sleep(self.poll_interval)
                    continue

                # ExecutionEngineì—ì„œ ì™„ë£Œëœ ì‘ì—… í™•ì¸ ë° ì²˜ë¦¬
                completed_jobs = self.execution_engine.check_and_process_completed_works(timeout=1.0)

                if completed_jobs:
                    logger.debug(f"Processed {len(completed_jobs)} completed jobs")

                pending_snapshot = self.execution_engine.get_pending_works_snapshot()
                if not pending_snapshot:
                    time.sleep(self.poll_interval)
                else:
                    time.sleep(1.0)

            except Exception as e:
                logger.error(f"âŒ Error in Ray monitor thread: {str(e)}", exc_info=True)
                time.sleep(self.poll_interval)

        logger.info("âš¡ Ray Work Monitor stopped")

    def start(self):
        """Daemon ì‹œì‘"""
        if self.run_once:
            logger.info("Starting EventDrivenLogger (run_once mode)...")
        else:
            logger.info("Starting EventDrivenLogger (continuous mode)...")

        if self.running:
            logger.warning("Logger is already running")
            return

        self.running = True
        self.stop_event.clear()

        self.listener_thread = Thread(
            target=self._listener_thread_func,
            daemon=True,
            name="DBChangeListener"
        )
        self.listener_thread.start()

        self.monitor_thread = Thread(
            target=self._ray_monitor_thread_func,
            daemon=True,
            name="RayWorkMonitor"
        )
        self.monitor_thread.start()
        logger.info("âœ“ Ray Work Monitor thread started")

        # run_once ëª¨ë“œë©´ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        if self.run_once:
            self.listener_thread.join()
            self.monitor_thread.join()
            self.running = False
            logger.info("âœ… EventDrivenLogger completed (run_once mode)")
        else:
            logger.info("âœ… EventDrivenLogger started successfully (continuous mode)")

    def stop(self):
        """Daemon ì¤‘ì§€"""
        logger.info("Stopping EventDrivenDaemon...")

        self.running = False
        self.stop_event.set()

        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self.listener_thread:
            self.listener_thread.join(timeout=5)
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)

        logger.info("âœ… EventDrivenDaemon stopped")


def main(run_once: bool = False):
    """
    EventDrivenLogger ì‹¤í–‰
    
    Args:
        run_once: Trueë©´ 1íšŒë§Œ ì‹¤í–‰, Falseë©´ ê³„ì† ì‹¤í–‰ (ê¸°ë³¸ê°’)
    """
    initialize_logger()
    event_logger = EventDrivenLogger(poll_interval=2.0, run_once=run_once)
    event_logger.start()

    if run_once:
        # run_once ëª¨ë“œ: start()ê°€ ì™„ë£Œë˜ë©´ ì¢…ë£Œ
        logger.info("âœ… Run once mode completed")
    else:
        # continuous ëª¨ë“œ: ë©”ì¸ ìŠ¤ë ˆë“œ ìœ ì§€
        logger.info("ì´ë²¤íŠ¸ ë¡œê±°ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘...")

        try:
            while True:
                time.sleep(1)
                # ì—¬ê¸°ì„œ ë‹¤ë¥¸ ì‘ì—… ê°€ëŠ¥
        except KeyboardInterrupt:
            logger.info("ì¢…ë£Œ ì‹ í˜¸ ë°›ìŒ...")
            event_logger.stop()


if __name__ == "__main__":
    import logging
    from sv.utils.logger import setup_common_logger

    # ray.init(num_cpus=8)
    ray.init(local_mode=True)
    run_once_mode = False

    # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    log_level = logging.DEBUG  # DEBUGë¡œ ì„¤ì •í•˜ë©´ ëª¨ë“  detail ë©”ì‹œì§€ ì¶œë ¥
    # log_level = logging.INFO   # INFOë¡œ ì„¤ì •í•˜ë©´ info ì´ìƒì˜ ë©”ì‹œì§€ë§Œ ì¶œë ¥

    # ê³µí†µ ë¡œê±° ì´ˆê¸°í™” (ë¡œê·¸ ë ˆë²¨ ì§€ì •)
    setup_common_logger(level=log_level)

    if run_once_mode:
        logger.info("ğŸ”„ Run once mode enabled")
    else:
        logger.info("â™¾ï¸ Continuous mode enabled")

    main(run_once=run_once_mode)
