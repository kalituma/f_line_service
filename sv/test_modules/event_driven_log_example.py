try:
    import ray  # type: ignore
except ImportError:
    ray = None  # type: ignore

from typing import Optional, Callable, List, Dict, Any
from threading import Thread, Event as ThreadEvent
import time

from sv.daemon.module.db_change_listener import DBChangeListener, DBChangeEvent, ChangeEventType
from sv.daemon.module.execution_engine import ExecutionEngine
from sv.daemon.module.job_manager import JobManager
from sv.daemon.module.task_manager import TaskManager
from sv.utils.logger import setup_logger, setup_common_logger
from sv.backend.service.service_manager import get_service_manager
from sv.daemon.module.update_handler import send_video_status_update
from sv.daemon.server_state import ServerAnalysisStatus
from sv.daemon.daemon_state import STATUS_FAILED, STATUS_SUCCESS, STATUS_NOT_EXISTS

from sv.test_modules.test_tasks import split_primary_task_result
from sv.test_modules.test_tasks.connection_task import ConnectionTask
from sv.test_modules.test_tasks.video_extract_task import VideoFrameExtractionTask
from sv.test_modules.test_tasks.segmentation_task import VideoSegmentationTask
from sv.test_modules.test_tasks.location_simulation_task import LocationSimulationTask
from sv.test_modules.test_tasks.feature_matching_task import FeatureMatchingTask
from sv.test_modules.test_tasks.geojson_boundary_task import SegmentationGeoJsonTask

logger = setup_logger(__name__)


def initialize_logger() -> None:
    setup_common_logger(None)


service_manager = get_service_manager()
if not service_manager.is_initialized():
    logger.info("ServiceManager ì´ˆê¸°í™” ì¤‘...")
    service_manager.initialize_all_services()


class EventDrivenLogger:
    def __init__(
            self,
            poll_interval: float = 2.0,
            num_executors: int = 2,
            run_once: bool = False,
    ):
        self.video_request_url = "http://127.0.0.1:8086/wildfire-data-sender/api/wildfire/sender"
        self.analysis_update_url = "http://127.0.0.1:8086/wildfire-data-receiver/api/wildfire/video-status"
        self.work_dir = "C:/__workspace/f_line_service/data/workspace"
        self.result_path = "C:/__workspace/f_line_service/data/vid/cy_all.geojson"

        self.job_manager = JobManager()
        self.task_manager = TaskManager()

        self.task_manager.register_primary_task(ConnectionTask(api_url=self.video_request_url))
        self.task_manager.register_secondary_tasks(
            [VideoFrameExtractionTask(), VideoSegmentationTask(), LocationSimulationTask(), FeatureMatchingTask(),
             SegmentationGeoJsonTask(result_path=self.result_path)])
        self.task_manager.set_data_splitter(split_primary_task_result)

        self.execution_engine = ExecutionEngine(base_work_dir=self.work_dir, update_url=self.analysis_update_url,
                                                num_executors=num_executors)

        self.db_listener = DBChangeListener(poll_interval)
        self.db_listener.on_change(self._handle_db_change)
        self.running = False
        self.run_once = run_once  # 1íšŒ ì‹¤í–‰ ëª¨ë“œ

        self.listener_thread: Optional[Thread] = None
        self.monitor_thread: Optional[Thread] = None

        self.poll_interval = poll_interval
        self.stop_event = ThreadEvent()

    ############################################## event processor ##############################################

    def _handle_db_change(self, event: DBChangeEvent) -> None:
        """
        DB ë³€ê²½ ì´ë²¤íŠ¸ ë‚´ë¶€ í•¸ë“¤ëŸ¬
        
        Args:
            event: DB ë³€ê²½ ì´ë²¤íŠ¸
        """
        logger.info(f"DB Change detected: {event}")

        # Job Queue INSERT ì´ë²¤íŠ¸ ì²˜ë¦¬
        if event.table_name == "job_queue" and event.event_type == ChangeEventType.PENDING_JOBS_DETECTED:
            logger.info(f"âœ“ New job detected: {event.data}")
            if self._on_job_created:
                try:
                    self._on_job_created()
                except Exception as e:
                    logger.error(f"Error in on_job_created callback: {str(e)}", exc_info=True)

    def check_changes(self) -> None:
        """
        Pending ìƒíƒœì˜ Job ê°œìˆ˜ í™•ì¸                    
        """
        try:
            self.db_listener.check_pending_jobs()
        except Exception as e:
            logger.error(f"Error checking pending jobs: {str(e)}")

    #########################################################################################################

    def _on_job_created(self) -> None:
        """Job ìƒì„± ê°ì§€ ì‹œ í•¸ë“¤ëŸ¬ (Event Processorì—ì„œ í˜¸ì¶œ)"""
        logger.info("Job creation event detected")
        self._process_pending_jobs()

    def _on_job_complete(self, job_id: str, result: Dict[str, Any]) -> None:
        """Job ì™„ë£Œ ì‹œ í•¸ë“¤ëŸ¬ (Event Processorì—ì„œ í˜¸ì¶œ)"""
        logger.info(f"Job {job_id} completed")

        # Job ìƒíƒœì— ë”°ë¼ ì²˜ë¦¬
        status = result.get('status')
        if status == 'failed':
            error_msg = result.get('error', 'Unknown error')
            logger.error(f"Job {job_id} failed: {error_msg}")
        else:
            self._update_video_status(job_id, result)

    def _update_video_status(self, job_id: str, result: Dict[str, Any]) -> None:
        """
        Job ìƒíƒœë¥¼ ì„œë²„ì— ì—…ë°ì´íŠ¸
        
        Args:
            job_id: Job ID
            result: Jobì˜ ê²°ê³¼ ì •ë³´ (status í¬í•¨)
        """
        try:
            loop_context = result.get('loop_context')
            frfr_id = loop_context.get('frfr_id')
            analysis_id = loop_context.get('analysis_id')            

            if not frfr_id or not analysis_id:
                logger.error(f"âŒ Missing frfr_id or analysis_id for job {job_id}")
                return
            
            video_updates = []
            videos_results = result.get('item_results')
            for video_result in videos_results:
                video = video_result.get('data_item')
                video_name = video.get('video_name')
                status = video_result.get('status')
                if status == STATUS_SUCCESS.to_str():
                    status_code = ServerAnalysisStatus.STAT_003.to_code()  # fline_extracted
                elif status == STATUS_FAILED.to_str():
                    status_code = ServerAnalysisStatus.STAT_004.to_code()  # fline_failed
                elif status == STATUS_NOT_EXISTS.to_str():
                    status_code = ServerAnalysisStatus.STAT_005.to_code()  # video_receive_failed
                else:
                    logger.error(f"âŒ Unknown status for video {video_name}: {status}")
                    continue
                video_updates.append({
                    "video_name": video_name,
                    "analysis_status": status_code
                })

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            logger.info(f"ğŸ“¤ Sending job status for job {job_id} to server...")

            send_video_status_update(
                update_url=self.analysis_update_url,
                frfr_id=frfr_id,
                analysis_id=analysis_id,
                video_updates=video_updates
            )

            logger.info(f"âœ… Job {job_id} status sent to server")

        except Exception as e:
            logger.error(f"âŒ Error updating job status for {job_id}: {str(e)}", exc_info=True)

    def _process_pending_jobs(self) -> None:
        """
        ëŒ€ê¸° ì¤‘ì¸ Job ì²˜ë¦¬ (ë¹„ë™ê¸° ì½œë°± ë°©ì‹)
        
        Flow:
        1. ë‹¤ìŒ PENDING Job ê°€ì ¸ì˜¤ê¸°
        2. Task ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹)
        3. ì™„ë£Œ ì‹œ _on_job_complete ì½œë°± í˜¸ì¶œ
        """
        job_info = self.job_manager.get_next_pending_job()

        if not job_info:
            logger.debug("No pending jobs")
            return

        job_id = job_info['job_id']
        frfr_id = job_info['frfr_id']

        # Task ì‹¤í–‰ ì¤€ë¹„ í™•ì¸
        if not self.task_manager.are_tasks_ready():
            logger.error("âŒ Tasks not ready for execution")
            return

        # Task ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹ - ì½œë°± ë°©ì‹)
        self.execution_engine.execute_job(
            job_info=job_info,
            primary_task=self.task_manager.primary_task,
            secondary_tasks=self.task_manager.secondary_tasks,
            data_splitter=self.task_manager.data_splitter,
            on_complete=self._on_job_complete  # ì½œë°± í•¨ìˆ˜
        )

        logger.info(f"âœ“ Job {job_id} (frfr_id={frfr_id}) submitted")

    def _listener_thread_func(self):
        """DB ë³€ê²½ ê°ì§€ ë¦¬ìŠ¤ë„ˆ ìŠ¤ë ˆë“œ"""
        if self.run_once:
            logger.info("ğŸ§ DB Change Listener started (run_once mode)")
        else:
            logger.info("ğŸ§ DB Change Listener started (continuous mode)")

        while not self.stop_event.is_set():
            try:
                # Job Queue í…Œì´ë¸” ê°ì‹œ (PENDING ìƒíƒœ)
                self.check_changes()

                # run_once ëª¨ë“œë©´ 1ë²ˆë§Œ ì‹¤í–‰í•˜ê³  ì¢…ë£Œ
                if self.run_once:
                    logger.info("âœ“ Run once mode: completed one check cycle")
                    break

                time.sleep(self.poll_interval)

            except Exception as e:
                logger.error(f"Error in listener thread: {str(e)}", exc_info=True)

                # run_once ëª¨ë“œì—ì„œë„ ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ
                if self.run_once:
                    break

                time.sleep(self.poll_interval)

        logger.info("ğŸ§ DB Change Listener stopped")

    def _ray_monitor_thread_func(self) -> None:
        """Ray Job ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œ (ExecutionEngineì˜ pending jobs ëª¨ë‹ˆí„°ë§)"""
        logger.info("âš¡ Ray Job Monitor started")

        while not self.stop_event.is_set():
            try:
                if not self.execution_engine:
                    time.sleep(self.poll_interval)
                    continue

                # ExecutionEngineì—ì„œ ì™„ë£Œëœ ì‘ì—… í™•ì¸ ë° ì²˜ë¦¬
                completed_jobs = self.execution_engine.check_and_process_completed_jobs(timeout=1.0)

                if completed_jobs:
                    logger.debug(f"Processed {len(completed_jobs)} completed jobs")

                pending_snapshot = self.execution_engine.get_pending_jobs_snapshot()
                if not pending_snapshot:
                    time.sleep(self.poll_interval)
                else:
                    time.sleep(1.0)

            except Exception as e:
                logger.error(f"âŒ Error in Ray monitor thread: {str(e)}", exc_info=True)
                time.sleep(self.poll_interval)

        logger.info("âš¡ Ray Job Monitor stopped")

    def start(self):
        """Daemon ì‹œì‘"""
        if self.run_once:
            logger.info("Starting EventDrivenLogger (run_once mode)...")
        else:
            logger.info("Starting EventDrivenLogger (continuous mode)...")

        if self.running:
            logger.warning("Logger is already running")
            return

        self.running = True
        self.stop_event.clear()

        self.listener_thread = Thread(
            target=self._listener_thread_func,
            daemon=True,
            name="DBChangeListener"
        )
        self.listener_thread.start()

        self.monitor_thread = Thread(
            target=self._ray_monitor_thread_func,
            daemon=True,
            name="RayJobMonitor"
        )
        self.monitor_thread.start()
        logger.info("âœ“ Ray Job Monitor thread started")

        # run_once ëª¨ë“œë©´ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        if self.run_once:
            self.listener_thread.join()
            self.monitor_thread.join()
            self.running = False
            logger.info("âœ… EventDrivenLogger completed (run_once mode)")
        else:
            logger.info("âœ… EventDrivenLogger started successfully (continuous mode)")

    def stop(self):
        """Daemon ì¤‘ì§€"""
        logger.info("Stopping EventDrivenDaemon...")

        self.running = False
        self.stop_event.set()

        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self.listener_thread:
            self.listener_thread.join(timeout=5)
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)

        logger.info("âœ… EventDrivenDaemon stopped")


def main(run_once: bool = False):
    """
    EventDrivenLogger ì‹¤í–‰
    
    Args:
        run_once: Trueë©´ 1íšŒë§Œ ì‹¤í–‰, Falseë©´ ê³„ì† ì‹¤í–‰ (ê¸°ë³¸ê°’)
    """
    initialize_logger()
    event_logger = EventDrivenLogger(poll_interval=2.0, run_once=run_once)
    event_logger.start()

    if run_once:
        # run_once ëª¨ë“œ: start()ê°€ ì™„ë£Œë˜ë©´ ì¢…ë£Œ
        logger.info("âœ… Run once mode completed")
    else:
        # continuous ëª¨ë“œ: ë©”ì¸ ìŠ¤ë ˆë“œ ìœ ì§€
        logger.info("ì´ë²¤íŠ¸ ë¡œê±°ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ ì¤‘...")

        try:
            while True:
                time.sleep(1)
                # ì—¬ê¸°ì„œ ë‹¤ë¥¸ ì‘ì—… ê°€ëŠ¥
        except KeyboardInterrupt:
            logger.info("ì¢…ë£Œ ì‹ í˜¸ ë°›ìŒ...")
            event_logger.stop()


if __name__ == "__main__":
    import logging
    from sv.utils.logger import setup_common_logger

    ray.init(local_mode=True)
    run_once_mode = True

    # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    log_level = logging.DEBUG  # DEBUGë¡œ ì„¤ì •í•˜ë©´ ëª¨ë“  detail ë©”ì‹œì§€ ì¶œë ¥
    # log_level = logging.INFO   # INFOë¡œ ì„¤ì •í•˜ë©´ info ì´ìƒì˜ ë©”ì‹œì§€ë§Œ ì¶œë ¥

    # ê³µí†µ ë¡œê±° ì´ˆê¸°í™” (ë¡œê·¸ ë ˆë²¨ ì§€ì •)
    setup_common_logger(level=log_level)

    if run_once_mode:
        logger.info("ğŸ”„ Run once mode enabled")
    else:
        logger.info("â™¾ï¸ Continuous mode enabled")

    main(run_once=run_once_mode)
