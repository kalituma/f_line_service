"""
Event-Driven Hybrid Daemon
- DB ë³€ê²½ì„ ê°ì§€í•˜ëŠ” ë¦¬ìŠ¤ë„ˆ ê¸°ë°˜ ì´ë²¤íŠ¸ ì²˜ë¦¬
- Job Queue í†µí•©
- Ray Actor í’€ ê´€ë¦¬
- ë¶€ë¶„ì  ì£¼ê¸°ì  í´ë§ ì§€ì›
"""

try:
    import ray  # type: ignore
except ImportError:
    ray = None  # type: ignore

import logging as logging_module
from datetime import datetime
from typing import List, Dict, Any, Optional, Callable
import asyncio
import time
from threading import Thread, Event as ThreadEvent
from fastapi import FastAPI  # type: ignore
from contextlib import asynccontextmanager

from sv.db.db_change_listener import DBChangeListener, DBChangeEvent, ChangeEventType
from sv.daemon.module.split_executor import FlineTaskSplitExecutor
from sv.task.task_base import TaskBase
from sv.backend.db.job_queue_db import JobQueue, JobStatus
from sv import LOG_DIR_PATH
from sv.utils.logger import setup_common_logger, setup_logger
from sv.backend.service.app_state import get_app_state_manager, AppState
from sv.backend.service.recovery_check import RecoveryCheckService

logger = setup_logger(__name__)
logging = logging_module

class FlineDaemon:
    """ì´ë²¤íŠ¸ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ Daemon"""
    
    def __init__(
        self,
        num_executors: int = 2,
        db_path: str = "jobs.db",
        poll_interval: float = 2.0,
        fallback_poll_interval: int = 30,
        enable_fallback_polling: bool = True,
        enable_event_listener: bool = True,
        primary_task: TaskBase = None,
        secondary_tasks: List[TaskBase] = None,
        data_splitter: Optional[Callable[[Dict[str, Any]], List[Any]]] = None,
        db_change_callback: Optional[Callable[[DBChangeEvent], None]] = None
    ):
        """
        Args:
            config: Daemon ì„¤ì •
            primary_task: ì²« ë²ˆì§¸ ì‹¤í–‰ ì‘ì—… (Task 1)
            secondary_tasks: ë°ì´í„° ë¶„í•  í›„ ê° ì•„ì´í…œì—ì„œ ì‹¤í–‰í•  ì‘ì—… ë¦¬ìŠ¤íŠ¸
            data_splitter: Task 1 ê²°ê³¼ë¥¼ ë¶„í• í•˜ëŠ” í•¨ìˆ˜
            db_change_callback: DB ë³€ê²½ ê°ì§€ ì‹œ ì‹¤í–‰í•  ì¶”ê°€ ì½œë°±
        """
        self.config = config
        self.primary_task = primary_task
        self.secondary_tasks = secondary_tasks or []
        self.data_splitter = data_splitter
        self.db_change_callback = db_change_callback
        
        # Job Queue ì´ˆê¸°í™”
        self.job_queue = JobQueue(config.db_path)
        self.job_queue._init_db()
        
        # DB ë³€ê²½ ë¦¬ìŠ¤ë„ˆ ì´ˆê¸°í™”
        self.db_listener = DBChangeListener(config.db_path, config.poll_interval)
        self.db_listener.on_change(self._handle_db_change)
        
        # Ray Actor í’€
        self.executor_actors = [
            FlineTaskSplitExecutor.remote(i) for i in range(config.num_executors)
        ]
        self.current_executor_idx = 0
        
        # ì œì–´ í”Œë˜ê·¸
        self.running = False
        self.listener_thread: Optional[Thread] = None
        self.fallback_poll_thread: Optional[Thread] = None
        self.stop_event = ThreadEvent()
        
        logger.info("=" * 80)
        logger.info("ğŸš€ EventDrivenDaemon initialized")
        logger.info(f"  Executors: {config.num_executors}")
        logger.info(f"  DB Path: {config.db_path}")
        logger.info(f"  Poll Interval: {config.poll_interval}s")
        logger.info(f"  Event Listener: {config.enable_event_listener}")
        logger.info(f"  Fallback Polling: {config.enable_fallback_polling}")
        logger.info("=" * 80)
    
    def register_primary_task(self, task: TaskBase):
        """ì£¼ìš” ì‘ì—… ë“±ë¡ (Task 1)"""
        self.primary_task = task
        logger.info(f"Primary task registered: {task.task_name}")
    
    def register_secondary_tasks(self, tasks: List[TaskBase]):
        """ë³´ì¡° ì‘ì—… ë“±ë¡ (Task 2~N)"""
        self.secondary_tasks.extend(tasks)
        logger.info(f"{len(tasks)} secondary tasks registered")
    
    def set_data_splitter(self, splitter: Callable[[Dict[str, Any]], List[Any]]):
        """ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ë“±ë¡"""
        self.data_splitter = splitter
        logger.info(f"Data splitter registered: {splitter.__name__}")
    
    def _handle_db_change(self, event: DBChangeEvent):
        """DB ë³€ê²½ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬"""
        logger.info(f"ğŸ”” DB Change detected: {event}")
        
        # ì¶”ê°€ ì½œë°± ì‹¤í–‰
        if self.db_change_callback:
            try:
                self.db_change_callback(event)
            except Exception as e:
                logger.error(f"Error in custom callback: {str(e)}", exc_info=True)
        
        # Job Queue í…Œì´ë¸” ë³€ê²½ ì‹œ
        if event.table_name == "job_queue" and event.event_type == ChangeEventType.INSERT:
            logger.info(f"New job detected: {event.data}")
            # ì¦‰ì‹œ ì²˜ë¦¬
            asyncio.create_task(self._process_pending_jobs())
    
    def _get_next_executor(self) -> ray.actor.ActorHandle:
        """ë‹¤ìŒ ì‹¤í–‰ì Actor ë°˜í™˜ (ë¼ìš´ë“œ ë¡œë¹ˆ)"""
        executor = self.executor_actors[self.current_executor_idx]
        self.current_executor_idx = (self.current_executor_idx + 1) % len(self.executor_actors)
        return executor
    
    async def _process_pending_jobs(self):
        """
        ëŒ€ê¸° ì¤‘ì¸ Job ì²˜ë¦¬
        
        Flow:
        1. Job Queueì—ì„œ ë‹¤ìŒ PENDING job ê°€ì ¸ì˜¤ê¸°
        2. Primary Task ì‹¤í–‰
        3. ê²°ê³¼ ë¶„í• 
        4. ê° ë°ì´í„°ì— ëŒ€í•´ Secondary Tasks ì‹¤í–‰
        5. ê²°ê³¼ ì €ì¥
        """
        try:
            # ì²˜ë¦¬í•  Jobì´ ìˆëŠ”ì§€ í™•ì¸
            job_id = self.job_queue.pop_next_job()
            
            if not job_id:
                logger.debug("No pending jobs")
                return
            
            logger.info("=" * 80)
            logger.info(f"ğŸ”„ Processing job: {job_id}")
            logger.info("=" * 80)
            
            # ==================== ì‘ì—… ì‹¤í–‰ ====================
            
            if not self.primary_task or not self.secondary_tasks:
                logger.error("Primary task or secondary tasks not registered")
                self.job_queue._conn().__enter__().execute(
                    "UPDATE job_queue SET status = ? WHERE job_id = ?",
                    (JobStatus.FAILED.value, job_id)
                )
                return
            
            loop_context = {
                'job_id': job_id,
                'start_time': datetime.now().isoformat(),
                'task_count': len(self.secondary_tasks)
            }
            
            executor = self._get_next_executor()
            
            logger.info(f"Submitting job {job_id} to executor with data splitting")
            
            # ë°ì´í„° ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰
            result_ref = executor.execute_with_data_splitting.remote(
                self.primary_task,
                self.secondary_tasks,
                loop_context,
                self.data_splitter or (lambda x: [x]),  # ê¸°ë³¸ ë¶„í•  í•¨ìˆ˜
                continue_on_error=True
            )
            
            # ê²°ê³¼ ëŒ€ê¸°
            result = ray.get(result_ref)
            
            # ê²°ê³¼ ì²˜ë¦¬
            await self._handle_execution_result(job_id, result)
            
            logger.info("=" * 80)
            logger.info(f"âœ… Job {job_id} completed")
            logger.info("=" * 80)
        
        except Exception as e:
            logger.error(f"âŒ Error processing pending jobs: {str(e)}", exc_info=True)
    
    async def _handle_execution_result(self, job_id: int, result: Dict[str, Any]):
        """ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬"""
        logger.info("Execution result:")
        logger.info(f"  Status: {result.get('status')}")
        logger.info(f"  Total duration: {result.get('total_duration'):.2f}s")
        logger.info(f"  Data items processed: {len(result.get('data_items', []))}")
        logger.info(f"  Errors: {result.get('error_count', 0)}")
        
        # Primary task ê²°ê³¼
        if result.get('primary_task'):
            primary = result['primary_task']
            logger.info(f"  Primary task: {primary['status']} ({primary.get('duration', 0):.2f}s)")
        
        # ê° ë°ì´í„° ì•„ì´í…œ ì²˜ë¦¬ ê²°ê³¼
        for item_idx, item_result in enumerate(result.get('data_items', []), 1):
            logger.info(f"  Data item {item_idx}: {item_result['status']} ({item_result.get('duration', 0):.2f}s)")
            for task_info in item_result.get('tasks', []):
                status_icon = "âœ“" if task_info['status'] == 'success' else "âœ—"
                logger.info(
                    f"    {status_icon} {task_info['task_name']}: "
                    f"{task_info['status']} ({task_info.get('duration', 0):.2f}s)"
                )
        
        # Job ìƒíƒœ ì—…ë°ì´íŠ¸
        status = JobStatus.COMPLETED if result['status'] == 'success' else JobStatus.FAILED
        
        try:
            with self.job_queue._conn() as conn:
                conn.execute(
                    "UPDATE job_queue SET status = ? WHERE job_id = ?",
                    (status.value, job_id)
                )
            logger.info(f"Job {job_id} status updated to {status.value}")
        except Exception as e:
            logger.error(f"Error updating job status: {str(e)}")
    
    def _listener_thread_func(self):
        """DB ë³€ê²½ ê°ì§€ ë¦¬ìŠ¤ë„ˆ ìŠ¤ë ˆë“œ"""
        logging.info("ğŸ§ DB Change Listener started")  # noqa: F541
        
        while not self.stop_event.is_set():
            try:
                # Job Queue í…Œì´ë¸” ê°ì‹œ (PENDING ìƒíƒœ)
                self.db_listener.check_status_column_change(
                    table_name="job_queue",
                    status_column="status",
                    target_status=JobStatus.PENDING.value,
                    id_column="job_id"
                )
                
                time.sleep(self.config.poll_interval)
            
            except Exception as e:
                logger.error(f"Error in listener thread: {str(e)}", exc_info=True)  # noqa: F541
                time.sleep(self.config.poll_interval)
        
        logger.info("ğŸ§ DB Change Listener stopped")
    
    def _fallback_poll_thread_func(self):
        """í´ë°± ì£¼ê¸°ì  í´ë§ ìŠ¤ë ˆë“œ"""
        logging.info("ğŸ“Š Fallback Polling started")  # noqa: F541
        
        while not self.stop_event.is_set():
            try:
                logger.debug("Running fallback periodic polling")
                asyncio.run(self._process_pending_jobs())
                
                time.sleep(self.config.fallback_poll_interval)
            
            except Exception as e:
                logger.error(f"Error in fallback polling: {str(e)}", exc_info=True)  # noqa: F541
                time.sleep(self.config.fallback_poll_interval)
        
        logger.info("ğŸ“Š Fallback Polling stopped")
    
    def start(self):
        """Daemon ì‹œì‘"""
        logger.info("Starting EventDrivenDaemon...")
        
        if self.running:
            logger.warning("Daemon is already running")
            return
        
        self.running = True
        self.stop_event.clear()
        
        # ë¦¬ìŠ¤ë„ˆ ìŠ¤ë ˆë“œ ì‹œì‘
        if self.config.enable_event_listener:
            self.listener_thread = Thread(
                target=self._listener_thread_func,
                daemon=True,
                name="DBChangeListener"
            )
            self.listener_thread.start()
        
        # í´ë°± í´ë§ ìŠ¤ë ˆë“œ ì‹œì‘
        if self.config.enable_fallback_polling:
            self.fallback_poll_thread = Thread(
                target=self._fallback_poll_thread_func,
                daemon=True,
                name="FallbackPoller"
            )
            self.fallback_poll_thread.start()
        
        logger.info("âœ… EventDrivenDaemon started successfully")
    
    def stop(self):
        """Daemon ì¤‘ì§€"""
        logger.info("Stopping EventDrivenDaemon...")
        
        self.running = False
        self.stop_event.set()
        
        # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
        if self.listener_thread:
            self.listener_thread.join(timeout=5)
        
        if self.fallback_poll_thread:
            self.fallback_poll_thread.join(timeout=5)
        
        logger.info("âœ… EventDrivenDaemon stopped")
    
    def get_status(self) -> Dict[str, Any]:
        """Daemon ìƒíƒœ ë°˜í™˜"""
        return {
            'running': self.running,
            'executors': len(self.executor_actors),
            'has_primary_task': self.primary_task is not None,
            'secondary_tasks': len(self.secondary_tasks),
            'listener_active': self.config.enable_event_listener and self.listener_thread and self.listener_thread.is_alive(),
            'fallback_polling_active': self.config.enable_fallback_polling and self.fallback_poll_thread and self.fallback_poll_thread.is_alive()
        }


# ==================== FastAPI í†µí•© ====================

@asynccontextmanager
async def lifespan_with_daemon(
    app: FastAPI
):
    """
    Daemonê³¼ í•¨ê»˜í•˜ëŠ” FastAPI ìƒëª…ì£¼ê¸° ê´€ë¦¬
    
    yield ì´ì „: ì‹œì‘ ì‹œ ì‹¤í–‰ (startup)
    yield ì´í›„: ì¢…ë£Œ ì‹œ ì‹¤í–‰ (shutdown)
    """
    # ==================== Startup ====================
    logger.info("=" * 80)
    logger.info("Server Startup: Initializing Daemon...")
    logger.info("=" * 80)
    
    app_state = get_app_state_manager()
    recovery_service = RecoveryCheckService()
    
    # ì´ˆê¸°í™” ì‘ì—…
    async def init_daemon():
        """Daemon ì´ˆê¸°í™”"""
        logger.info("Initializing daemon...")
        daemon.start()
        logger.info("Daemon initialized")
    
    async def init_recovery_check():
        """Recovery check ìˆ˜í–‰"""
        logger.info("Running recovery check...")
        await asyncio.sleep(0.5)
        logger.info("Recovery check completed")
    
    async def init_database():
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        logger.info("Initializing database...")
        await asyncio.sleep(0.5)
        logger.info("Database initialized")
    
    # ì´ˆê¸°í™” ì‘ì—… ë“±ë¡
    recovery_service.add_task("daemon", init_daemon)
    recovery_service.add_task("recovery_check", init_recovery_check)
    recovery_service.add_task("database", init_database)
    
    # ì´ˆê¸°í™” ì‘ì—… ì‹¤í–‰
    success = await recovery_service.run_all()
    
    if success:
        await app_state.set_state(AppState.READY)
        logger.info("=" * 80)
        logger.info("Server is READY to accept requests!")
        logger.info("=" * 80)
    else:
        logger.error("=" * 80)
        logger.error("Initialization FAILED!")
        logger.error("=" * 80)
        await app_state.set_state(AppState.SHUTDOWN)
        raise RuntimeError("Server initialization failed")
    
    yield  # ì„œë²„ ì‹¤í–‰ ì¤‘
    
    # ==================== Shutdown ====================
    logger.info("=" * 80)
    logger.info("Server Shutting down...")
    logger.info("=" * 80)
    
    daemon.stop()
    ray.shutdown()
    
    logger.info("Server shutdown completed")


def initialize_logger(log_dir_path=None):
    """ê³µí†µ ë¡œê±° ì´ˆê¸°í™”"""
    if log_dir_path is None:
        log_dir_path = LOG_DIR_PATH
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir_path / f"f_line_server_{timestamp}.log"
    setup_common_logger(log_file)
    
    return log_file


if __name__ == '__main__':
    # 1. ë¡œê±° ì´ˆê¸°í™”
    log_file = initialize_logger()
    
    # 2. Ray ì´ˆê¸°í™”
    ray.init(num_cpus=8, ignore_reinit_error=True)
    
    # 3. Daemon ì„¤ì •
    config = EventDrivenDaemonConfig(
        num_executors=5,
        db_path="data/sqlite/jobs.db",
        poll_interval=2.0,
        fallback_poll_interval=30,
        enable_event_listener=True,
        enable_fallback_polling=True
    )
    
    # 4. Daemon ìƒì„±
    daemon = EventDrivenDaemon(config)
    
    # âœ… TODO: Task ë“±ë¡ (ì‚¬ìš©ìê°€ êµ¬í˜„í•´ì•¼ í•¨)
    # from sv.task.your_tasks import Task1, Task2, Task3
    # 
    # def split_result(result):
    #     return result.get('items', [])
    # 
    # daemon.register_primary_task(Task1())
    # daemon.register_secondary_tasks([Task2(), Task3()])
    # daemon.set_data_splitter(split_result)
    
    logger.info("Daemon configured and ready to use")
    logger.info(f"Daemon status: {daemon.get_status()}")

