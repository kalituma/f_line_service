"""
Event-Driven Daemon ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ
3ë‹¨ê³„ë¡œ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!
"""

# ============================================================
# STEP 1: Task ì •ì˜ (your_tasks.py)
# ============================================================

from sv.task.task_base import TaskBase
from typing import Dict, Any
import time


class Step1_CollectData(TaskBase):
    """Step 1: ë°ì´í„° ìˆ˜ì§‘ (Task 1)"""
    
    def __init__(self):
        super().__init__("collect_data")
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘"""
        self.logger.info("Collecting data...")
        
        # ì‹¤ì œë¡œëŠ” DBì—ì„œ ì¡°íšŒ
        items = [
            {'id': 1, 'name': 'Item 1', 'value': 100},
            {'id': 2, 'name': 'Item 2', 'value': 200},
            {'id': 3, 'name': 'Item 3', 'value': 300},
        ]
        
        return {'items': items}


class Step2_ProcessItem(TaskBase):
    """Step 2: ê° ì•„ì´í…œ ì²˜ë¦¬ (Task 2)"""
    
    def __init__(self):
        super().__init__("process_item")
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œë³„ ì•„ì´í…œ ì²˜ë¦¬"""
        item = context.get('current_data')
        item_id = item.get('id')
        
        self.logger.info(f"Processing item {item_id}")
        time.sleep(1)  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        
        return {
            'processed': True,
            'item_id': item_id,
            'result': item['value'] * 2
        }


class Step3_SaveResult(TaskBase):
    """Step 3: ê²°ê³¼ ì €ì¥ (Task 3)"""
    
    def __init__(self):
        super().__init__("save_result")
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ ì €ì¥"""
        item = context.get('current_data')
        process_result = context.get('process_item', {})
        
        item_id = item.get('id')
        
        self.logger.info(f"Saving result for item {item_id}")
        time.sleep(0.5)  # ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
        
        return {
            'saved': True,
            'item_id': item_id,
            'saved_value': process_result.get('result')
        }


# ============================================================
# STEP 2: ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ì •ì˜
# ============================================================

def split_items(result: Dict[str, Any]):
    """
    Task 1ì˜ ê²°ê³¼ì—ì„œ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    (ê° ì•„ì´í…œë§ˆë‹¤ Task 2, 3ì„ ìˆœì°¨ ì‹¤í–‰)
    """
    items = result.get('items', [])
    print(f"ğŸ“¦ Splitting into {len(items)} items for processing")
    return items


# ============================================================
# STEP 3: Daemon ìƒì„± ë° ì‹¤í–‰
# ============================================================

def main_event_driven():
    """Event-Driven Daemon ì‹¤í–‰"""
    
    import ray
    from sv.backup.event_driven_daemon import EventDrivenDaemon, EventDrivenDaemonConfig
    import logging
    
    # Ray ì´ˆê¸°í™”
    ray.init(num_cpus=4, ignore_reinit_error=True)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 80)
    logger.info("ğŸš€ Event-Driven Daemon ì‹œì‘")
    logger.info("=" * 80)
    
    # 1ï¸âƒ£ Daemon ì„¤ì •
    config = EventDrivenDaemonConfig(
        num_executors=3,
        db_path="data/sqlite/jobs.db",
        poll_interval=2.0,
        fallback_poll_interval=30,
        enable_event_listener=True,
        enable_fallback_polling=True
    )
    
    # 2ï¸âƒ£ Daemon ìƒì„±
    daemon = EventDrivenDaemon(config)
    
    # 3ï¸âƒ£ Task ë“±ë¡
    daemon.register_primary_task(Step1_CollectData())
    daemon.register_secondary_tasks([
        Step2_ProcessItem(),
        Step3_SaveResult()
    ])
    daemon.set_data_splitter(split_items)
    
    # 4ï¸âƒ£ Daemon ì‹œì‘
    daemon.start()
    
    logger.info("âœ… Daemon ì‹œì‘ë¨")
    logger.info(f"Status: {daemon.get_status()}")
    
    # 5ï¸âƒ£ Job ìƒì„± (í…ŒìŠ¤íŠ¸)
    logger.info("\nğŸ“ Job ìƒì„± ì¤‘...")
    job_id = daemon.job_queue.add_job("fire_001", "analysis_001")
    
    if job_id:
        logger.info(f"âœ… Job ìƒì„±ë¨: job_id={job_id}")
        
        # Jobì´ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°
        import time as time_module
        time_module.sleep(15)  # 15ì´ˆ ëŒ€ê¸°
        
        logger.info("\nğŸ“Š ìµœì¢… ê²°ê³¼ (ë¡œê·¸ í™•ì¸)")
    
    # 6ï¸âƒ£ Daemon ì¢…ë£Œ
    logger.info("\nğŸ›‘ Daemon ì¢…ë£Œ ì¤‘...")
    daemon.stop()
    ray.shutdown()
    
    logger.info("âœ… ì™„ë£Œ!")


# ============================================================
# FastAPIì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì˜ˆì œ
# ============================================================

def main_fastapi():
    """FastAPI + Event-Driven Daemon í†µí•© ì˜ˆì œ"""
    
    import ray
    from fastapi import FastAPI
    from contextlib import asynccontextmanager
    import uvicorn
    from sv.backup.event_driven_daemon import EventDrivenDaemon, EventDrivenDaemonConfig
    import logging
    
    # Ray ì´ˆê¸°í™”
    ray.init(num_cpus=4, ignore_reinit_error=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Daemon ìƒì„±
    config = EventDrivenDaemonConfig(
        num_executors=3,
        db_path="data/sqlite/jobs.db"
    )
    
    daemon = EventDrivenDaemon(config)
    daemon.register_primary_task(Step1_CollectData())
    daemon.register_secondary_tasks([
        Step2_ProcessItem(),
        Step3_SaveResult()
    ])
    daemon.set_data_splitter(split_items)
    
    # FastAPI ì•± ìƒì„±
    @asynccontextmanager
    async def lifespan(app: FastAPI):
        # Startup
        logger = logging.getLogger(__name__)
        logger.info("Starting FastAPI + Daemon...")
        daemon.start()
        
        yield  # ì•± ì‹¤í–‰ ì¤‘
        
        # Shutdown
        logger.info("Stopping FastAPI + Daemon...")
        daemon.stop()
        ray.shutdown()
    
    app = FastAPI(title="Event-Driven Service", lifespan=lifespan)
    
    # API ì—”ë“œí¬ì¸íŠ¸
    @app.get("/health")
    async def health():
        return {"status": "ok", "daemon": daemon.get_status()}
    
    @app.post("/jobs")
    async def create_job(frfr_id: str, analysis_id: str):
        job_id = daemon.job_queue.add_job(frfr_id, analysis_id)
        if job_id:
            return {"job_id": job_id, "status": "created"}
        else:
            return {"error": "Failed to create job"}, 400
    
    @app.get("/jobs/{job_id}")
    async def get_job(job_id: int):
        with daemon.job_queue._conn() as conn:
            row = conn.execute(
                "SELECT * FROM job_queue WHERE job_id = ?",
                (job_id,)
            ).fetchone()
            if row:
                return dict(row)
            else:
                return {"error": "Job not found"}, 404
    
    # ì‹¤í–‰
    uvicorn.run(app, host="0.0.0.0", port=8090)


# ============================================================
# CLI ì‚¬ìš© ì˜ˆì œ
# ============================================================

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "fastapi":
        print("\nğŸš€ FastAPI ëª¨ë“œë¡œ ì‹œì‘")
        main_fastapi()
    else:
        print("\nğŸš€ Standalone ëª¨ë“œë¡œ ì‹œì‘")
        main_event_driven()

"""
ì‹¤í–‰ ë°©ë²•:

1. Standalone ëª¨ë“œ (ì§ì ‘ ì‹¤í–‰):
   $ python sv/daemon_v2_quickstart.py

2. FastAPI ëª¨ë“œ:
   $ python sv/daemon_v2_quickstart.py fastapi
   
   ê·¸ í›„ í„°ë¯¸ë„ì—ì„œ:
   $ curl http://localhost:8090/health
   $ curl -X POST "http://localhost:8090/jobs?frfr_id=fire_001&analysis_id=ana_001"
   $ curl http://localhost:8090/jobs/1

ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ (ë¡œê·¸):
   2024-01-07 10:00:00 - __main__ - INFO - ğŸš€ Event-Driven Daemon ì‹œì‘
   2024-01-07 10:00:00 - EventDrivenDaemon - INFO - ğŸ§ DB Change Listener started
   2024-01-07 10:00:00 - EventDrivenDaemon - INFO - ğŸ“Š Fallback Polling started
   2024-01-07 10:00:01 - __main__ - INFO - ğŸ“ Job ìƒì„± ì¤‘...
   2024-01-07 10:00:01 - __main__ - INFO - âœ… Job ìƒì„±ë¨: job_id=1
   
   (Event triggered)
   
   2024-01-07 10:00:02 - EventDrivenDaemon - INFO - ğŸ”„ Processing job: 1
   2024-01-07 10:00:02 - Executor-0 - INFO - Executor 0 executing with data splitting
   2024-01-07 10:00:02 - Task-collect_data - INFO - Collecting data...
   2024-01-07 10:00:03 - Task-collect_data - INFO - [1] Primary task 'collect_data' completed
   2024-01-07 10:00:03 - Task-collect_data - INFO - [2] Splitting data from primary task result
   2024-01-07 10:00:03 - Task-collect_data - INFO - [2] Data split into 3 items
   
   2024-01-07 10:00:03 - Task-process_item - INFO - [3.1/3.1] Executing task: process_item
   2024-01-07 10:00:04 - Task-process_item - INFO - Processing item 1
   2024-01-07 10:00:05 - Task-save_result - INFO - [3.1/3.1.2] Executing task: save_result
   2024-01-07 10:00:05 - Task-save_result - INFO - Saving result for item 1
   2024-01-07 10:00:05 - Task-save_result - INFO - âœ… Result saved
   
   [Item 2, 3ë„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬...]
   
   2024-01-07 10:00:20 - EventDrivenDaemon - INFO - âœ… Job 1 completed
   2024-01-07 10:00:20 - __main__ - INFO - âœ… ì™„ë£Œ!
"""

