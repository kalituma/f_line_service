"""
Event-Driven Daemon ì‚¬ìš© ì˜ˆì œ

ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤:
1. Task1: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¶„ì„í•  ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ
2. Task2: ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ AI ë¶„ì„ ìˆ˜í–‰
3. Task3: ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥

Flow:
DB Update (new job) 
  â†’ Event Listener ê°ì§€ 
    â†’ Task1 ì‹¤í–‰ (ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ) 
      â†’ ê²°ê³¼ ë¶„í•  (ê° ë¹„ë””ì˜¤ë³„ë¡œ) 
        â†’ ê° ë¹„ë””ì˜¤ì— ëŒ€í•´ Task2, Task3 ìˆœì°¨ ì‹¤í–‰
"""

import logging
from typing import Dict, Any, List
import time

from sv.task.task_base import TaskBase
from sv.backup.event_driven_daemon import EventDrivenDaemon, EventDrivenDaemonConfig
from fastapi import FastAPI
import uvicorn
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)


# ==================== Task êµ¬í˜„ ì˜ˆì œ ====================

class FetchVideoListTask(TaskBase):
    """Task 1: ë¶„ì„í•  ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ"""
    
    def __init__(self):
        super().__init__("fetch_video_list")
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ PENDING ìƒíƒœì˜ ë¹„ë””ì˜¤ ëª©ë¡ ì¡°íšŒ
        
        Args:
            context: ì´ì „ Task ê²°ê³¼
            
        Returns:
            ë¹„ë””ì˜¤ ëª©ë¡
        """
        self.logger.info(f"Fetching video list... (Job ID: {context.get('job_id', 'N/A')})")
        
        # ì‹œë®¬ë ˆì´ì…˜: DBì—ì„œ ë°ì´í„° ì¡°íšŒ
        time.sleep(1)
        
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ DB ì¿¼ë¦¬ ìˆ˜í–‰
        videos = [
            {
                'video_id': 'vid_001',
                'file_path': '/data/videos/fire_001.mp4',
                'location': 'Seoul',
                'timestamp': '2024-01-01T10:00:00'
            },
            {
                'video_id': 'vid_002',
                'file_path': '/data/videos/fire_002.mp4',
                'location': 'Busan',
                'timestamp': '2024-01-01T11:00:00'
            },
            {
                'video_id': 'vid_003',
                'file_path': '/data/videos/fire_003.mp4',
                'location': 'Incheon',
                'timestamp': '2024-01-01T12:00:00'
            }
        ]
        
        self.logger.info(f"Fetched {len(videos)} videos")
        
        return {
            'status': 'success',
            'videos': videos,
            'count': len(videos)
        }


class AnalyzeVideoTask(TaskBase):
    """Task 2: ê° ë¹„ë””ì˜¤ ë¶„ì„"""
    
    def __init__(self):
        super().__init__("analyze_video")
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¹„ë””ì˜¤ì— ëŒ€í•´ AI ë¶„ì„ ìˆ˜í–‰
        
        Args:
            context: í˜„ì¬ ë°ì´í„° ì•„ì´í…œ í¬í•¨í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        video = context.get('current_data', {})
        video_id = video.get('video_id', 'unknown')
        
        self.logger.info(f"Analyzing video: {video_id}")
        
        # ì‹œë®¬ë ˆì´ì…˜: AI ë¶„ì„ ìˆ˜í–‰
        time.sleep(2)
        
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ AI ëª¨ë¸ ì‹¤í–‰
        analysis_result = {
            'video_id': video_id,
            'fire_detected': True,
            'confidence': 0.95,
            'affected_area': 45.2,  # hectares
            'frame_count': 1200
        }
        
        self.logger.info(f"Analysis complete for {video_id}: confidence={analysis_result['confidence']}")
        
        return {
            'status': 'success',
            'analysis': analysis_result
        }


class SaveAnalysisResultTask(TaskBase):
    """Task 3: ë¶„ì„ ê²°ê³¼ ì €ì¥"""
    
    def __init__(self):
        super().__init__("save_analysis_result")
    
    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        
        Args:
            context: í˜„ì¬ ë°ì´í„° ì•„ì´í…œê³¼ ì´ì „ Task ê²°ê³¼ í¬í•¨í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ì €ì¥ ê²°ê³¼
        """
        video = context.get('current_data', {})
        analysis_result = context.get('analyze_video', {}).get('analysis', {})
        
        video_id = video.get('video_id', 'unknown')
        
        self.logger.info(f"Saving analysis result for video: {video_id}")
        
        # ì‹œë®¬ë ˆì´ì…˜: DB ì €ì¥
        time.sleep(0.5)
        
        # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ DBì— INSERT/UPDATE
        saved_data = {
            'video_id': video_id,
            'analysis_id': f"ana_{video_id}_{int(time.time())}",
            'saved_at': '2024-01-01T13:00:00',
            'status': 'completed'
        }
        
        self.logger.info(f"Analysis result saved for {video_id}")
        
        return {
            'status': 'success',
            'saved': saved_data
        }


# ==================== ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ====================

def split_videos(result: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Task 1ì˜ ê²°ê³¼ì—ì„œ ë¹„ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„í• 
    
    Args:
        result: Task 1ì˜ ì‹¤í–‰ ê²°ê³¼
        
    Returns:
        ê°œë³„ ë¹„ë””ì˜¤ ëª©ë¡
    """
    videos = result.get('videos', [])
    logger.info(f"Splitting {len(videos)} videos for parallel processing")
    return videos


# ==================== FastAPI + Daemon í†µí•© ====================

# Daemon ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
config = EventDrivenDaemonConfig(
    num_executors=3,
    db_path="data/sqlite/jobs.db",
    poll_interval=2.0,
    fallback_poll_interval=30,
    enable_event_listener=True,
    enable_fallback_polling=True
)

daemon = EventDrivenDaemon(config)

# Task ë“±ë¡
daemon.register_primary_task(FetchVideoListTask())
daemon.register_secondary_tasks([
    AnalyzeVideoTask(),
    SaveAnalysisResultTask()
])
daemon.set_data_splitter(split_videos)


# ì»¤ìŠ¤í…€ DB ë³€ê²½ ì½œë°± (ì„ íƒì‚¬í•­)
def custom_db_change_handler(event):
    """DB ë³€ê²½ ê°ì§€ ì‹œ ì¶”ê°€ ì²˜ë¦¬"""
    logger.info(f"Custom handler: {event.table_name} {event.event_type.value}")


# FastAPI ì•± ìƒì„±
@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI ìƒëª…ì£¼ê¸°"""
    # Startup
    logger.info("Starting FastAPI app with Event-Driven Daemon...")
    daemon.start()
    
    yield  # ì•± ì‹¤í–‰ ì¤‘
    
    # Shutdown
    logger.info("Shutting down FastAPI app...")
    daemon.stop()


app = FastAPI(
    title="F-Line Service (Event-Driven)",
    description="ì•¼ì‚°ë¶ˆ ê°ì§€ ì„œë¹„ìŠ¤ - ì´ë²¤íŠ¸ ê¸°ë°˜ Daemon",
    lifespan=lifespan
)


# ==================== API Endpoints ====================

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        'status': 'healthy',
        'daemon': daemon.get_status()
    }


@app.post("/api/jobs")
async def create_job(frfr_id: str, analysis_id: str):
    """
    ìƒˆë¡œìš´ Job ìƒì„±
    
    Args:
        frfr_id: ì‚°ë¶ˆ ì •ë³´ ID
        analysis_id: ë¶„ì„ ID
        
    Returns:
        ìƒì„±ëœ Job ì •ë³´
    """
    try:
        job_id = daemon.job_queue.add_job(frfr_id, analysis_id)
        
        if job_id:
            return {
                'status': 'created',
                'job_id': job_id,
                'frfr_id': frfr_id,
                'analysis_id': analysis_id
            }
        else:
            return {
                'status': 'error',
                'message': 'Failed to create job (duplicate or error)'
            }, 400
    
    except Exception as e:
        logger.error(f"Error creating job: {str(e)}")
        return {
            'status': 'error',
            'message': str(e)
        }, 500


@app.get("/api/jobs/{job_id}")
async def get_job_status(job_id: int):
    """
    Job ìƒíƒœ ì¡°íšŒ
    
    Args:
        job_id: Job ID
        
    Returns:
        Job ìƒíƒœ ì •ë³´
    """
    try:
        with daemon.job_queue._conn() as conn:
            row = conn.execute(
                "SELECT job_id, frfr_id, analysis_id, status, created_at FROM job_queue WHERE job_id = ?",
                (job_id,)
            ).fetchone()
            
            if row:
                return {
                    'job_id': row['job_id'],
                    'frfr_id': row['frfr_id'],
                    'analysis_id': row['analysis_id'],
                    'status': row['status'],
                    'created_at': row['created_at']
                }
            else:
                return {
                    'status': 'error',
                    'message': f'Job {job_id} not found'
                }, 404
    
    except Exception as e:
        logger.error(f"Error getting job status: {str(e)}")
        return {
            'status': 'error',
            'message': str(e)
        }, 500


@app.get("/api/daemon/status")
async def daemon_status():
    """Daemon ìƒíƒœ ì¡°íšŒ"""
    return daemon.get_status()


# ==================== ë©”ì¸ ====================

if __name__ == '__main__':
    import ray
    
    # Ray ì´ˆê¸°í™”
    ray.init(num_cpus=8, ignore_reinit_error=True)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 80)
    logger.info("ğŸš€ Starting F-Line Service (Event-Driven Daemon)")
    logger.info("=" * 80)
    
    # Uvicornìœ¼ë¡œ FastAPI ì•± ì‹¤í–‰
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8090,
        log_level="info"
    )

