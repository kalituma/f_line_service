from typing import Dict, Any, Optional
from datetime import datetime

try:
    import ray  # type: ignore
except ImportError:
    ray = None  # type: ignore

from sv import LOG_DIR_PATH
from sv.utils.logger import setup_common_logger, setup_logger

from sv.backend.service.app_state_manager import get_app_state_manager, AppState
from sv.backend.service.service_manager import get_service_manager

from sv.daemon.module.recovery_check import RecoveryCheckService
from sv.daemon.module.work_manager import WorkManager
from sv.daemon.module.task_manager import TaskManager
from sv.daemon.module.event_processor import EventProcessor
from sv.daemon.module.execution_engine import ExecutionEngine
from sv.daemon.module.thread_manager import ThreadManager
from sv.daemon.module.update_handler import send_video_status_update
from sv.daemon.module.http_request_client import post_request, HttpRequestError

from sv.daemon.server_state import ServerAnalysisStatus
from sv.daemon.daemon_state import STATUS_FAILED, STATUS_SUCCESS, STATUS_NOT_EXISTS

from sv.routers.fline_web_server import init_web_server

logger = setup_logger(__name__)

class FlineDaemon:
    """
    ì´ë²¤íŠ¸ ê¸°ë°˜ Daemon
    """

    def __init__(
        self,
        base_work_dir: str,
        analysis_update_url,
        result_sent_url,
        num_executors: int = 2,
        poll_interval: float = 2.0,
        web_host: str = "localhost",
        web_port: int = 8090
    ):
        """
        Args:
            num_executors: Ray Actor ê°œìˆ˜
            poll_interval: Event Listener í´ë§ ê°„ê²©
        """
        self.analysis_update_url = analysis_update_url
        self.result_sent_url = result_sent_url

        self.service_manager = get_service_manager()
        self.recovery_service = RecoveryCheckService()
        self.setup_recovery_tasks()

        self.web_app = init_web_server(self)
        self.app_state = get_app_state_manager()

        # ==================== ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ====================
        self.work_manager = WorkManager()
        self.task_manager = TaskManager()

        self.event_processor = EventProcessor(
            poll_interval=poll_interval,
            on_work_created=self._on_work_created
        ) # including listener

        # ExecutionEngine ë¨¼ì € ì´ˆê¸°í™”
        self.execution_engine = ExecutionEngine(base_work_dir, update_url=self.analysis_update_url, num_executors=num_executors)
        
        # ThreadManager ì´ˆê¸°í™” ì‹œ ExecutionEngine ì „ë‹¬
        self.thread_manager = ThreadManager(
            poll_interval=poll_interval,
            web_app=self.web_app,
            web_host=web_host,
            web_port=web_port,
            execution_engine=self.execution_engine  # Ray Job Monitorë¥¼ ìœ„í•´ ì „ë‹¬
        )
        
        # Thread Manager ì½œë°± ì„¤ì •
        self.thread_manager.set_check_changes_callback(
            self.event_processor.check_changes
        )
        
        logger.info("=" * 80)
        logger.info("   FlineDaemon initialized")
        logger.info(f"  Executors: {num_executors}")
        logger.info(f"  Poll Interval: {poll_interval}s")
        logger.info("=" * 80)
    
    # ==================== Task Management API ====================
    
    def register_primary_task(self, task):
        """Primary Task ë“±ë¡"""
        self.task_manager.register_primary_task(task)
    
    def register_secondary_tasks(self, tasks):
        """Secondary Tasks ë“±ë¡"""
        self.task_manager.register_secondary_tasks(tasks)
    
    def set_data_splitter(self, splitter):
        """ë°ì´í„° ë¶„í•  í•¨ìˆ˜ ë“±ë¡"""
        self.task_manager.set_data_splitter(splitter)

    def setup_recovery_tasks(self):
        """RecoveryCheckServiceì— ì´ˆê¸°í™” ìž‘ì—… ë“±ë¡"""

        # Task 1: Processing to Pending ì¤€ë¹„
        async def change_works_to_pending():
            """
            ì‹œìŠ¤í…œ ë³µêµ¬ ì‹œ PROCESSING ìƒíƒœì˜ ëª¨ë“  ìž‘ì—…ì„ PENDINGìœ¼ë¡œ ë³€ê²½
            (ë¹„ì •ìƒ ì¢…ë£Œ í›„ ìž¬ì‹œìž‘ ì‹œ ì§„í–‰ ì¤‘ì´ë˜ ìž‘ì—… ë³µêµ¬)
            """
            logger.info("=" * 80)
            logger.info("ðŸ”„ Recovering works from PROCESSING to PENDING...")
            logger.info("=" * 80)

            try:
                from sv.backend.work_status import WorkStatus

                # WorkQueueService ê°€ì ¸ì˜¤ê¸°
                work_queue_service = self.service_manager.get_work_queue_service()
                if not work_queue_service:
                    logger.error("âŒ WorkQueueService not available")
                    return False

                # PROCESSING ìƒíƒœì˜ ëª¨ë“  ìž‘ì—… ì¡°íšŒ
                logger.info("ðŸ“‹ Fetching works with PROCESSING status...")
                processing_works = work_queue_service.get_works_by_status(WorkStatus.PROCESSING)

                if not processing_works:
                    logger.info("âœ“ No works in PROCESSING status")
                    return True

                logger.info(f"ðŸ“Š Found {len(processing_works)} works in PROCESSING status")

                # PROCESSING ìƒíƒœì˜ ìž‘ì—…ì„ PENDINGìœ¼ë¡œ ë³€ê²½
                success_count = 0
                failed_count = 0

                for work in processing_works:
                    work_id = work.get('work_id')
                    logger.info(f"  â†’ Updating work_id={work_id} to PENDING...")

                    try:
                        result = work_queue_service.update_work_status(work_id, WorkStatus.PENDING)
                        if result:
                            success_count += 1
                            logger.info(f"    âœ“ work_id={work_id} changed to PENDING")
                        else:
                            failed_count += 1
                            logger.warning(f"    âœ— Failed to change work_id={work_id} status")
                    except Exception as e:
                        failed_count += 1
                        logger.error(f"    âœ— Error updating work_id={work_id}: {str(e)}")

                logger.info("=" * 80)
                logger.info(f"âœ“ Recovery complete: {success_count} succeeded, {failed_count} failed")
                logger.info("=" * 80)

                return failed_count == 0

            except Exception as e:
                logger.error("=" * 80)
                logger.error(f"âŒ Error during work recovery: {str(e)}")
                logger.error("=" * 80)
                return False

        # ì´ˆê¸°í™” ìž‘ì—… ë“±ë¡
        self.recovery_service.add_task("change_works_to_pending", change_works_to_pending)

    # ==================== Internal Event Handlers ====================
    
    def _on_work_created(self) -> None:
        """Job ìƒì„± ê°ì§€ ì‹œ í•¸ë“¤ëŸ¬ (Event Processorì—ì„œ í˜¸ì¶œ)"""
        logger.info("Job creation event detected")
        self._process_pending_work()
    
    def _process_pending_work(self) -> None:
        """
        ëŒ€ê¸° ì¤‘ì¸ Job ì²˜ë¦¬ (ë¹„ë™ê¸° ì½œë°± ë°©ì‹)
        
        Flow:
        1. ë‹¤ìŒ PENDING Job ê°€ì ¸ì˜¤ê¸°
        2. Task ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹)
        3. ì™„ë£Œ ì‹œ _on_job_complete ì½œë°± í˜¸ì¶œ
        """
        work_info = self.work_manager.get_next_pending_work()

        if not work_info:
            logger.debug("No pending works")
            return

        work_id = work_info['work_id']
        frfr_id = work_info['frfr_id']

        # Task ì‹¤í–‰ ì¤€ë¹„ í™•ì¸
        if not self.task_manager.are_tasks_ready():
            logger.error("âŒ Tasks not ready for execution")
            return

        # Task ì‹¤í–‰ (ë…¼ë¸”ë¡œí‚¹ - ì½œë°± ë°©ì‹)
        self.execution_engine.execute_work(
            work_info=work_info,
            primary_task=self.task_manager.primary_task,
            secondary_tasks=self.task_manager.secondary_tasks,
            data_splitter=self.task_manager.data_splitter,
            on_job_complete=self._on_work_complete  # ì½œë°± í•¨ìˆ˜
        )

        logger.info(f"âœ“ Work {work_id} (frfr_id={frfr_id}) submitted")

    def _on_work_complete(self, work_id: str, result: Dict[str, Any]) -> None:
        """Work ì™„ë£Œ ì‹œ í•¸ë“¤ëŸ¬ (Event Processorì—ì„œ í˜¸ì¶œ)"""
        logger.info(f"Work {work_id} completed")

        # Work ìƒíƒœì— ë”°ë¼ ì²˜ë¦¬
        status = result.get('status')
        if status == 'failed':
            error_msg = result.get('error', 'Unknown error')
            logger.error(f"Work {work_id} failed: {error_msg}")
        else:
            self._update_video_status(work_id, result)
            self._send_analysis_results(work_id, result)

    def _update_video_status(self, work_id: str, result: Dict[str, Any]) -> None:
        """
        Work ìƒíƒœë¥¼ ì„œë²„ì— ì—…ë°ì´íŠ¸

        Args:
            work_id: Work ID
            result: Workì˜ ê²°ê³¼ ì •ë³´ (status í¬í•¨)
        """
        try:
            loop_context = result.get('loop_context')
            frfr_id = loop_context.get('frfr_id')
            analysis_id = loop_context.get('analysis_id')

            if not frfr_id or not analysis_id:
                logger.error(f"âŒ Missing frfr_id or analysis_id for work {work_id}")
                return

            video_updates = []
            videos_results = result.get('item_results')
            for video_result in videos_results:
                video = video_result.get('data_item')
                video_name = video.get('video_name')
                status = video_result.get('status')
                if status == STATUS_SUCCESS.to_str():
                    status_code = ServerAnalysisStatus.STAT_003.to_code()  # fline_extracted
                elif status == STATUS_FAILED.to_str():
                    status_code = ServerAnalysisStatus.STAT_004.to_code()  # fline_failed
                elif status == STATUS_NOT_EXISTS.to_str():
                    status_code = ServerAnalysisStatus.STAT_005.to_code()  # video_receive_failed
                else:
                    logger.error(f"âŒ Unknown status for video {video_name}: {status}")
                    continue
                video_updates.append({
                    "video_name": video_name,
                    "analysis_status": status_code
                })

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            logger.info(f"ðŸ“¤ Sending work status for work {work_id} to server...")

            send_video_status_update(
                update_url=self.analysis_update_url,
                frfr_id=frfr_id,
                analysis_id=analysis_id,
                video_updates=video_updates
            )

            logger.info(f"âœ… Work {work_id} status sent to server")

        except Exception as e:
            logger.error(f"âŒ Error updating work status for {work_id}: {str(e)}", exc_info=True)

    def _send_analysis_results(self, work_id: str, result: Dict[str, Any]) -> None:
        """
        ì„±ê³µí•œ ìž‘ì—…ì˜ SegmentationGeoJsonTask ê²°ê³¼ë¥¼ ì„œë²„ë¡œ ì „ì†¡

        Args:
            work_id: Work ID
            result: Workì˜ ê²°ê³¼ ì •ë³´
        """
        try:
            loop_context = result.get('loop_context')
            frfr_id = loop_context.get('frfr_id')
            analysis_id = loop_context.get('analysis_id')

            if not frfr_id or not analysis_id:
                logger.error(f"âŒ Missing frfr_id or analysis_id for work {work_id}")
                return

            # item_resultsì—ì„œ ì„±ê³µí•œ í•­ëª©ë§Œ ì²˜ë¦¬
            videos_results = result.get('item_results', [])
            sent_count = 0

            for video_result in videos_results:
                status = video_result.get('status')

                # statusê°€ 'success'ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
                if status != STATUS_SUCCESS.to_str():
                    continue

                # tasksì—ì„œ SegmentationGeoJsonTask ê²°ê³¼ ì°¾ê¸°
                tasks = video_result.get('tasks', [])
                segmentation_task_result = None

                for task in tasks:
                    if task.get('task_name') == 'SegmentationGeoJsonTask':
                        segmentation_task_result = task.get('result')
                        break

                if not segmentation_task_result:
                    logger.warning("âš ï¸  No SegmentationGeoJsonTask result found for video")
                    continue

                # integrated_convex_hull ê°€ì ¸ì˜¤ê¸°
                integrated_convex_hull = segmentation_task_result.get('integrated_convex_hull')

                if not integrated_convex_hull:
                    logger.warning("âš ï¸  No integrated_convex_hull found in SegmentationGeoJsonTask result")
                    continue

                # GeoJSONì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                enriched_geojson = integrated_convex_hull.copy()

                # frfr_idë¥¼ frfr_info_idë¡œ ì¶”ê°€
                enriched_geojson['frfr_info_id'] = frfr_id

                # analysis_id ì¶”ê°€
                enriched_geojson['analysis_id'] = analysis_id

                # í˜„ìž¬ ì‹œê°„ì„ yyyy-MM-dd HH:mm:ss í˜•íƒœë¡œ ì¶”ê°€
                current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                enriched_geojson['timestamp'] = current_time

                # ì„œë²„ë¡œ POST ì „ì†¡
                try:
                    logger.info(f"ðŸ“¤ Sending analysis result to {self.result_sent_url}...")
                    logger.debug(f"   frfr_info_id: {frfr_id}")
                    logger.debug(f"   analysis_id: {analysis_id}")
                    logger.debug(f"   timestamp: {current_time}")

                    response = post_request(
                        url=self.result_sent_url,
                        json_data=enriched_geojson,
                        headers={'Content-Type': 'application/json'},
                        timeout=30
                    )

                    logger.info("âœ… Analysis result sent successfully")
                    logger.debug(f"   Response: {response.text}")
                    sent_count += 1

                except HttpRequestError as e:
                    logger.error(f"âŒ HTTP request error sending analysis result: {str(e)}")
                except Exception as e:
                    logger.error(f"âŒ Error sending analysis result: {str(e)}", exc_info=True)

            if sent_count > 0:
                logger.info(f"âœ… Sent {sent_count} analysis result(s) for work {work_id}")
            else:
                logger.info(f"â„¹ï¸  No analysis results to send for work {work_id}")

        except Exception as e:
            logger.error(f"âŒ Error in _send_analysis_results for {work_id}: {str(e)}", exc_info=True)

    async def restore_and_init(self) -> bool:
        """
        ë³µêµ¬ ë° ì´ˆê¸°í™” ìˆ˜í–‰ (RecoveryCheckService ì‹¤í–‰)

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        logger.info("=" * 80)
        logger.info("Starting Daemon Initialization...")
        logger.info("=" * 80)

        logger.info("=" * 80)
        logger.info("Initializing all services...")
        logger.info("=" * 80)

        service_init_success = self.service_manager.initialize_all_services()

        if not service_init_success:
            await self.app_state.set_state(AppState.SHUTDOWN)
            logger.error("=" * 80)
            logger.error("âŒ Service Initialization FAILED!")
            logger.error("=" * 80)
            return False

        # RecoveryCheckServiceì˜ ëª¨ë“  ìž‘ì—… ì‹¤í–‰
        success = await self.recovery_service.run_all()

        if success:
            await self.app_state.set_state(AppState.READY)
            logger.info("=" * 80)
            logger.info("âœ… Daemon is READY!")
            logger.info("=" * 80)
        else:
            await self.app_state.set_state(AppState.SHUTDOWN)
            logger.error("=" * 80)
            logger.error("âŒ Daemon Initialization FAILED!")
            logger.error("=" * 80)

        return success

    # ==================== Lifecycle Management ====================
    
    def start(self) -> None:
        """Daemon ì‹œìž‘"""
        logger.info("Starting FlineDaemon...")
        self.thread_manager.start()
        logger.info("FlineDaemon started successfully")
    
    def stop(self) -> None:
        """Daemon ì¤‘ì§€"""
        logger.info("Stopping FlineDaemon...")
        self.thread_manager.stop()
        logger.info("FlineDaemon stopped")