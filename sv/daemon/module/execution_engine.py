from typing import Dict, Any, Callable, Optional, Tuple, List
from datetime import datetime
from threading import Lock
import os

try:
    import ray  # type: ignore
except ImportError:
    ray = None  # type: ignore

from sv.daemon.server_state import ServerAnalysisStatus
from sv.daemon.daemon_state import JobExecutionStatus
from sv.daemon.module.split_executor import FlineTaskSplitExecutor
from sv.daemon.module.http_request_client import HttpRequestError
from sv.daemon.module.update_handler import send_video_status_update
from sv.backend.service.service_manager import get_service_manager
from sv.backend.job_status import JobStatus
from sv.task.task_base import TaskBase
from sv.utils.logger import setup_logger

logger = setup_logger(__name__)

class ExecutionEngine:
    """Task ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬ (ìŠ¤ë ˆë“œ ê´€ë¦¬ëŠ” ThreadManagerê°€ ë‹´ë‹¹)"""
    
    def __init__(self, base_work_dir: str, update_url: str, num_executors: int = 2):
        """
        Args:
            num_executors: Ray Actor ê°œìˆ˜
            base_work_dir: ì‘ì—… ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.num_executors = num_executors
        self.base_work_dir = base_work_dir
        self.update_url = update_url
        self.job_queue_service = get_service_manager().get_job_queue_service()

        # base_work_dirì´ ì—†ìœ¼ë©´ ìƒì„±
        try:
            os.makedirs(base_work_dir, exist_ok=True)
            logger.info(f"âœ“ Base work directory created/exists: {base_work_dir}")
        except Exception as e:
            logger.error(f"âŒ Failed to create base work directory: {str(e)}", exc_info=True)
            raise
        
        self.executor_actors = [
            FlineTaskSplitExecutor.remote(i, base_work_dir) for i in range(num_executors)
        ]
        self.current_executor_idx = 0
        
        # ë¹„ë™ê¸° ì‘ì—… ê´€ë¦¬ (ray.wait() ê¸°ë°˜)
        self.pending_jobs: Dict[int, ray.ObjectRef] = {}  # job_id -> ObjectRef
        self.job_callbacks: Dict[int, Callable] = {}  # job_id -> callback
        self.lock = Lock()
        
        logger.info(f"âœ“ ExecutionEngine initialized with {num_executors} executors")

    def _create_pre_secondary_callback(self, update_url: str) -> Callable:
        """
        ê° ë°ì´í„° ì•„ì´í…œë³„ë¡œ Secondary tasks ì‹¤í–‰ ì§ì „ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì½œë°± ìƒì„±

        Args:
            update_url: ë°ì´í„° ì—…ë°ì´íŠ¸ ìš”ì²­ì„ ë³´ë‚¼ URL

        Returns:
            ì½œë°± í•¨ìˆ˜
        """
        def pre_secondary_callback(data_item: Dict[str, Any], loop_context: Dict[str, Any]) -> None:
            """
            ê° ë°ì´í„° ì•„ì´í…œì— ëŒ€í•´ secondary tasks ì‹¤í–‰ ì§ì „ í˜¸ì¶œë˜ëŠ” ì½œë°±

            Args:
                data_item: í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ ë°ì´í„° ì•„ì´í…œ (video_name, video_url í¬í•¨)
                loop_context: Jobì˜ ë£¨í”„ ì»¨í…ìŠ¤íŠ¸ (job_id, frfr_id, analysis_id ë“±)
            """
            try:
                frfr_id = loop_context.get('frfr_id')
                analysis_id = loop_context.get('analysis_id')
                job_id = loop_context.get('job_id')

                # data_itemì—ì„œ video_nameê³¼ video_url ì¶”ì¶œ
                video_name = data_item if isinstance(data_item, str) else data_item.get('video_name', 'unknown')
                video_url = data_item.get('video_url') if isinstance(data_item, dict) else ''

                logger.info("=" * 80)
                logger.info(f"ğŸ“¤ Pre-secondary callback ì‹¤í–‰: Job {job_id}")
                logger.info(f"   URL: {update_url}")
                logger.info(f"   frfr_id: {frfr_id}, analysis_id: {analysis_id}")
                logger.info(f"   video_name: {video_name}")
                logger.info(f"   video_url: {video_url}")
                logger.info("=" * 80)

                # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                analysis_status = ServerAnalysisStatus.STAT_002
                if not os.path.isfile(video_url):
                    logger.warning(f"âš ï¸  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {video_url}")
                    analysis_status = ServerAnalysisStatus.STAT_005                        
                    file_exists = False
                else:
                    file_exists = True
                    logger.info(f"âœ“ íŒŒì¼ ì¡´ì¬ í™•ì¸: {video_url}")

                # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ STAT_002ë¡œ ì—…ë°ì´íŠ¸
                send_video_status_update(
                    update_url=update_url,
                    frfr_id=frfr_id,
                    analysis_id=analysis_id,
                    video_updates=[
                        {
                            "video_name": video_name,
                            "analysis_status": analysis_status.to_code()
                        }
                    ]
                )

                if not file_exists:
                    raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {video_url}")

            except HttpRequestError as e:
                logger.error(f"âŒ ë°ì´í„° ì—…ë°ì´íŠ¸ ìš”ì²­ ì‹¤íŒ¨: {str(e)}", exc_info=True)
                raise
            except Exception as e:
                logger.error(f"âŒ ì½œë°± ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {str(e)}", exc_info=True)
                raise

        return pre_secondary_callback
    
    def _get_next_executor(self):
        """
        ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ Executor ë°˜í™˜
        
        Returns:
            Ray Actor Handle
        """
        executor = self.executor_actors[self.current_executor_idx]
        self.current_executor_idx = (self.current_executor_idx + 1) % len(self.executor_actors)
        return executor
    
    # ==================== Job ìƒíƒœ ê´€ë¦¬ ë©”ì„œë“œ ====================
    
    def _update_job_status_safe(self, job_id: int, status: JobStatus, log_prefix: str = "") -> bool:
        """
        Job ìƒíƒœë¥¼ ì•ˆì „í•˜ê²Œ ì—…ë°ì´íŠ¸ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)
        
        Args:
            job_id: Job ID
            status: ë³€ê²½í•  ìƒíƒœ
            log_prefix: ë¡œê·¸ ë©”ì‹œì§€ ì•ì— ë¶™ì¼ ì ‘ë‘ì‚¬
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.job_queue_service.update_job_status(job_id, status)
            logger.info(f"{log_prefix}âœ“ Job {job_id} status updated to {status}")
            return True
        except Exception as e:
            logger.error(f"{log_prefix}âŒ Failed to update job {job_id} status to {status}: {str(e)}", exc_info=True)
            return False
    
    def _on_job_start(self, job_id: int) -> bool:
        """
        Job ì‹œì‘ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            job_id: Job ID
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        return self._update_job_status_safe(job_id, JobStatus.PROCESSING, "ğŸš€ ")
    
    def _on_job_complete(self, job_id: int, result: Dict[str, Any]) -> None:
        """
        Job ì™„ë£Œ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            job_id: Job ID
            result: Job ì‹¤í–‰ ê²°ê³¼
        """
        status = result.get('status', 'failed')
        job_status = JobStatus.COMPLETED if status == 'success' else JobStatus.FAILED
        self._update_job_status_safe(job_id, job_status, "ğŸ ")
    
    def execute_job(
        self,
        job_info: Dict[str, Any],
        primary_task: TaskBase,
        secondary_tasks: list,
        data_splitter,
        on_complete: Optional[Callable[[int, Dict[str, Any]], None]] = None
    ) -> None:
        """
        Job ì‹¤í–‰ (ë¹„ë™ê¸° ì½œë°± ë°©ì‹)
        
        Args:
            job_info: Job       ì •ë³´ ë”•ì…”ë„ˆë¦¬
            primary_task: Primary Task
            secondary_tasks: Secondary Tasks ë¦¬ìŠ¤íŠ¸
            data_splitter: ë°ì´í„° ë¶„í•  í•¨ìˆ˜
            on_complete: ì™„ë£Œ ì‹œ í˜¸ì¶œë  ì½œë°± í•¨ìˆ˜ (job_id, result)
        """
        job_id = job_info['job_id']
        
        # Job ì‹œì‘ - ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ì—…ë°ì´íŠ¸
        if not self._on_job_start(job_id):
            logger.error("âŒ Failed to update job status to PROCESSING")
            return

        try:
            if not primary_task or not secondary_tasks:
                logger.error("âŒ Primary task or secondary tasks not registered")
                error_result = {
                    **job_info,
                    'status': JobExecutionStatus.FAILED.to_str(),
                    'error': 'Tasks not registered'
                }
                if on_complete:
                    on_complete(job_id, error_result)
                return
            
            logger.info("=" * 80)
            logger.info(f"ğŸ”„ Submitting job: {job_id}")
            logger.info("=" * 80)
            
            # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±            
            loop_context = {
                **job_info,
                'start_time': datetime.now().strftime('%Y%m%dT%H%M%S'),
                'task_count': len(secondary_tasks)
            }
            
            executor = self._get_next_executor()
            
            logger.info(f"Submitting job {job_id} to executor with data splitting")

            # Pre-secondary ì½œë°± ìƒì„±
            pre_secondary_callback = self._create_pre_secondary_callback(self.update_url)

            # Ray Actorì—ì„œ ë°ì´í„° ë¶„í•  ë°©ì‹ ì‹¤í–‰
            result_ref = executor.execute_with_data_splitting.remote(
                primary_task,
                secondary_tasks,
                loop_context,
                data_splitter or (lambda x: [x]),
                continue_on_error=True,
                pre_secondary_callback=pre_secondary_callback
            )
            
            # ObjectRefì™€ ì½œë°±ì„ pending_jobsì— ë“±ë¡
            with self.lock:
                self.pending_jobs[job_id] = result_ref
                if on_complete:
                    self.job_callbacks[job_id] = on_complete
            
            logger.info(f"âœ“ Job {job_id} submitted successfully (monitoring by ThreadManager)")
            
        except Exception as e:
            logger.error(f"âŒ Error executing job {job_id}: {str(e)}", exc_info=True)
            error_result = {
                'job_id': job_id,
                'status': JobExecutionStatus.FAILED.to_str(),
                'error': str(e)
            }
            if on_complete:
                on_complete(job_id, error_result)
    
    # ==================== ëª¨ë‹ˆí„°ë§ ì¸í„°í˜ì´ìŠ¤ (ThreadManagerê°€ í˜¸ì¶œ) ====================
    
    def get_pending_jobs_snapshot(self) -> Dict[int, ray.ObjectRef]:
        """
        í˜„ì¬ pending jobsì˜ ìŠ¤ëƒ…ìƒ· ë°˜í™˜ (ThreadManagerì˜ ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©)
        
        Returns:
            {job_id: ObjectRef} ë”•ì…”ë„ˆë¦¬
        """
        with self.lock:
            return dict(self.pending_jobs)
    
    def check_and_process_completed_jobs(self, timeout: float = 1.0) -> List[Tuple[int, Dict[str, Any]]]:
        """
        ì™„ë£Œëœ ì‘ì—…ì„ í™•ì¸í•˜ê³  ì²˜ë¦¬ (ThreadManagerì˜ ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œ)
        
        Args:
            timeout: ray.wait() íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            
        Returns:
            ì™„ë£Œëœ ì‘ì—… ë¦¬ìŠ¤íŠ¸ [(job_id, result), ...]
        """
        with self.lock:
            if not self.pending_jobs:
                return []
            
            job_refs_map = dict(self.pending_jobs)
        
        if not job_refs_map:
            return []
        
        completed_jobs = []
        
        try:
            # ray.wait()ë¡œ ì™„ë£Œëœ ì‘ì—… í™•ì¸
            object_refs = list(job_refs_map.values())
            ready_refs, _ = ray.wait(
                object_refs,
                num_returns=len(object_refs),
                timeout=timeout
            )
            
            # readyëœ ì‘ì—…ë“¤ ì²˜ë¦¬
            for ready_ref in ready_refs:
                # ObjectRefì— í•´ë‹¹í•˜ëŠ” job_id ì°¾ê¸°
                completed_job_id = None
                for jid, ref in job_refs_map.items():
                    if ref == ready_ref:
                        completed_job_id = jid
                        break
                
                if completed_job_id is None:
                    continue
                
                try:
                    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                    result = ray.get(ready_ref)
                    
                    logger.info("=" * 80)
                    logger.info(f"âœ… Job {completed_job_id} execution completed: {result.get('status')}")
                    logger.info("=" * 80)

                    # Job ì™„ë£Œ - ìƒíƒœë¥¼ COMPLETED/FAILEDë¡œ ì—…ë°ì´íŠ¸
                    self._on_job_complete(completed_job_id, result)

                    # ì½œë°± í˜¸ì¶œ
                    with self.lock:
                        callback = self.job_callbacks.get(completed_job_id)
                    
                    if callback:
                        callback(completed_job_id, result)
                    
                    completed_jobs.append((completed_job_id, result))
                    
                except Exception as e:
                    logger.error(f"âŒ Error processing job {completed_job_id}: {str(e)}", exc_info=True)
                    error_result = {
                        'job_id': completed_job_id,
                        'status': JobExecutionStatus.FAILED.to_str(),
                        'error': str(e)
                    }
                    
                    with self.lock:
                        callback = self.job_callbacks.get(completed_job_id)
                    
                    if callback:
                        callback(completed_job_id, error_result)
                    
                    completed_jobs.append((completed_job_id, error_result))
                
                finally:
                    # pending_jobsì—ì„œ ì œê±°
                    with self.lock:
                        self.pending_jobs.pop(completed_job_id, None)
                        self.job_callbacks.pop(completed_job_id, None)
            
        except Exception as e:
            logger.error(f"Error checking completed jobs: {str(e)}", exc_info=True)
        
        return completed_jobs
    
    def log_execution_result(self, job_id: int, result: Dict[str, Any]) -> None:
        """
        ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
        
        Args:
            job_id: Job ID
            result: ì‹¤í–‰ ê²°ê³¼
        """
        logger.info("ğŸ“Š Execution Result:")
        logger.info(f"  Job ID: {job_id}")
        logger.info(f"  Status: {result.get('status')}")
        logger.info(f"  Duration: {result.get('total_duration', 0):.2f}s")
        logger.info(f"  Data items: {len(result.get('data_items', []))}")
        logger.info(f"  Errors: {result.get('error_count', 0)}")
        
        # Primary Task ê²°ê³¼
        if result.get('primary_task'):
            primary = result['primary_task']
            logger.info(f"  Primary Task: {primary['status']} ({primary.get('duration', 0):.2f}s)")
        
        # ê° ë°ì´í„° ì•„ì´í…œ ì²˜ë¦¬ ê²°ê³¼
        for item_idx, item_result in enumerate(result.get('data_items', []), 1):
            logger.info(f"  Item {item_idx}: {item_result['status']} ({item_result.get('duration', 0):.2f}s)")
            for task_info in item_result.get('tasks', []):
                status_icon = "âœ“" if task_info['status'] == 'success' else "âœ—"
                logger.info(
                    f"    {status_icon} {task_info['task_name']}: "
                    f"{task_info['status']} ({task_info.get('duration', 0):.2f}s)"
                )
    
    def get_executor_status(self) -> Dict[str, Any]:
        """
        Executor ìƒíƒœ ë°˜í™˜
        
        Returns:
            Executor ìƒíƒœ ì •ë³´
        """
        with self.lock:
            pending_count = len(self.pending_jobs)
        
        return {
            'num_executors': self.num_executors,
            'current_executor_idx': self.current_executor_idx,
            'pending_jobs_count': pending_count
        }

