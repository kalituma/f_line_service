from typing import Dict, Any, Callable, Optional, Tuple, List
from datetime import datetime
from threading import Lock
import os

try:
    import ray  # type: ignore
except ImportError:
    ray = None  # type: ignore


from sv.daemon.daemon_state import JobExecutionStatus
from sv.daemon.module.split_executor import FlineTaskSplitExecutor
from sv.daemon.module.fault_injector import FaultInjector
from sv.backend.service.service_manager import get_service_manager
from sv.backend.work_status import WorkStatus
from sv.task.task_base import TaskBase
from sv.utils.logger import setup_logger

logger = setup_logger(__name__)

class ExecutionEngine:
    """Task ì‹¤í–‰ ë° ê²°ê³¼ ì²˜ë¦¬ (ìŠ¤ë ˆë“œ ê´€ë¦¬ëŠ” ThreadManagerê°€ ë‹´ë‹¹)"""
    
    def __init__(self, base_work_dir: str, update_url: str, num_executors: int = 2):
        """
        Args:
            num_executors: Ray Actor ê°œìˆ˜
            base_work_dir: ì‘ì—… ê¸°ë³¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.num_executors = num_executors
        self.base_work_dir = base_work_dir
        self._work_queue_service = None

        # base_work_dirì´ ì—†ìœ¼ë©´ ìƒì„±
        try:
            os.makedirs(base_work_dir, exist_ok=True)
            logger.info(f"âœ“ Base work directory created/exists: {base_work_dir}")
        except Exception as e:
            logger.error(f"âŒ Failed to create base work directory: {str(e)}", exc_info=True)
            raise

        fault_injector = FaultInjector()
    
    
        # fault_injector.register_fault(
        #     method_name="_on_secondary_tasks_start",
        #     exception=FileNotFoundError("raised intentional file not found error"),
        #     probability=1.0
        # )
        self.executor_actors = [
            FlineTaskSplitExecutor.remote(i, base_work_dir, update_url, fault_injector) for i in range(num_executors)
        ]
        self.current_executor_idx = 0
        
        # ë¹„ë™ê¸° ì‘ì—… ê´€ë¦¬ (ray.wait() ê¸°ë°˜)
        self.pending_works: Dict[int, ray.ObjectRef] = {}  # work_id -> ObjectRef
        self.work_callbacks: Dict[int, Callable] = {}  # work_id -> callback
        self.lock = Lock()
        
        logger.info(f"âœ“ ExecutionEngine initialized with {num_executors} executors")

    @property
    def work_queue_service(self):
        if self._work_queue_service is None:
            self._work_queue_service = get_service_manager().get_work_queue_service()
        return self._work_queue_service

    def _get_next_executor(self):
        """
        ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ Executor ë°˜í™˜
        
        Returns:
            Ray Actor Handle
        """
        executor = self.executor_actors[self.current_executor_idx]
        self.current_executor_idx = (self.current_executor_idx + 1) % len(self.executor_actors)
        return executor
    
    # ==================== Work ìƒíƒœ ê´€ë¦¬ ë©”ì„œë“œ ====================
    
    def _update_work_status_safe(self, work_id: int, status: WorkStatus) -> bool:
        """
        Work ìƒíƒœë¥¼ ì•ˆì „í•˜ê²Œ ì—…ë°ì´íŠ¸ (ì—ëŸ¬ í•¸ë“¤ë§ í¬í•¨)
        
        Args:
            work_id: Work ID
            status: ë³€ê²½í•  ìƒíƒœ
            log_prefix: ë¡œê·¸ ë©”ì‹œì§€ ì•ì— ë¶™ì¼ ì ‘ë‘ì‚¬
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.work_queue_service.update_work_status(work_id, status)
            logger.info(f"âœ“ Work {work_id} status updated to {status}")
            return True
        except Exception as e:
            logger.error(f"âŒ Failed to update work {work_id} status to {status}: {str(e)}", exc_info=True)
            return False
    
    def _on_work_start(self, work_id: int) -> bool:
        """
        Work ì‹œì‘ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            work_id: Work ID
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        return self._update_work_status_safe(work_id, WorkStatus.PROCESSING)
    
    def _on_execution_complete(self, work_id: int, result: Dict[str, Any]) -> None:
        """
        Work ì™„ë£Œ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
        
        Args:
            work_id: Work ID
            result: Work ì‹¤í–‰ ê²°ê³¼
        """
        status = result.get('status', 'failed')
        error_count = result.get('error_count', 0)
        
        # statusê°€ 'success'ì´ê³  error_countê°€ 0ì¼ ë•Œë§Œ COMPLETED
        # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ FAILEDë¡œ ì²˜ë¦¬
        if status == 'success' and error_count == 0:
            work_status = WorkStatus.COMPLETED
            logger.info(f"Work {work_id} completed successfully (status={status}, errors={error_count})")
        else:
            work_status = WorkStatus.FAILED
            logger.warning(f"Work {work_id} marked as FAILED (status={status}, errors={error_count})")
            
        self._update_work_status_safe(work_id, work_status)
    
    def execute_work(
        self,
        work_info: Dict[str, Any],
        primary_task: TaskBase,
        secondary_tasks: list,
        data_splitter,
        on_job_complete: Optional[Callable[[int, Dict[str, Any]], None]] = None
    ) -> None:
        """
        Work ì‹¤í–‰ (ë¹„ë™ê¸° ì½œë°± ë°©ì‹)
        
        Args:
            work_info: Work       ì •ë³´ ë”•ì…”ë„ˆë¦¬
            primary_task: Primary Task
            secondary_tasks: Secondary Tasks ë¦¬ìŠ¤íŠ¸
            data_splitter: ë°ì´í„° ë¶„í•  í•¨ìˆ˜
            on_job_complete: ì™„ë£Œ ì‹œ í˜¸ì¶œë  ì½œë°± í•¨ìˆ˜ (work_id, result)
        """
        work_id = work_info['work_id']
        
        # Work ì‹œì‘ - ìƒíƒœë¥¼ PROCESSINGìœ¼ë¡œ ì—…ë°ì´íŠ¸
        if not self._on_work_start(work_id):
            logger.error("âŒ Failed to update work status to PROCESSING")
            return

        try:
            if not primary_task or not secondary_tasks:
                logger.error("âŒ Primary task or secondary tasks not registered")
                self._update_work_status_safe(work_id, WorkStatus.FAILED)

                error_result = {
                    **work_info,
                    'status': JobExecutionStatus.FAILED.to_str(),
                    'error': 'Tasks not registered'
                }
                if on_job_complete:
                    on_job_complete(work_id, error_result)
                return
            
            logger.info("=" * 80)
            logger.info(f"ğŸ”„ Submitting work: {work_id}")
            logger.info("=" * 80)
            
            # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìƒì„±            
            loop_context = {
                **work_info,
                'start_time': datetime.now().strftime('%Y%m%dT%H%M%S'),
                'task_count': len(secondary_tasks)
            }
            
            executor = self._get_next_executor()
            
            logger.info(f"Submitting work {work_id} to executor with data splitting")

            result_ref = executor.execute_with_data_splitting.remote(
                primary_task,
                secondary_tasks,
                loop_context,
                data_splitter or (lambda x: [x]),
                continue_on_error=True,
            )
            
            # ObjectRefì™€ ì½œë°±ì„ pending_worksì— ë“±ë¡
            with self.lock:
                self.pending_works[work_id] = result_ref
                if on_job_complete:
                    self.work_callbacks[work_id] = on_job_complete
            
            logger.info(f"âœ“ Work {work_id} submitted successfully (monitoring by ThreadManager)")
            
        except Exception as e:
            logger.error(f"âŒ Error executing work {work_id}: {str(e)}", exc_info=True)            
            self._update_work_status_safe(work_id, WorkStatus.FAILED)
            error_result = {
                'work_id': work_id,
                'status': JobExecutionStatus.FAILED.to_str(),
                'error': str(e)
            }
            if on_job_complete:
                on_job_complete(work_id, error_result)
    
    # ==================== ëª¨ë‹ˆí„°ë§ ì¸í„°í˜ì´ìŠ¤ (ThreadManagerê°€ í˜¸ì¶œ) ====================
    
    def get_pending_works_snapshot(self) -> Dict[int, ray.ObjectRef]:
        """
        í˜„ì¬ pending worksì˜ ìŠ¤ëƒ…ìƒ· ë°˜í™˜ (ThreadManagerì˜ ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œì—ì„œ ì‚¬ìš©)
        
        Returns:
            {work_id: ObjectRef} ë”•ì…”ë„ˆë¦¬
        """
        with self.lock:
            return dict(self.pending_works)
    
    def check_and_process_completed_works(self, timeout: float = 1.0) -> List[Tuple[int, Dict[str, Any]]]:
        """
        ì™„ë£Œëœ ì‘ì—…ì„ í™•ì¸í•˜ê³  ì²˜ë¦¬ (ThreadManagerì˜ ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œ)
        
        Args:
            timeout: ray.wait() íƒ€ì„ì•„ì›ƒ (ì´ˆ)
            
        Returns:
            ì™„ë£Œëœ ì‘ì—… ë¦¬ìŠ¤íŠ¸ [(work_id, result), ...]
        """
        with self.lock:
            if not self.pending_works:
                return []
            
            work_refs_map = dict(self.pending_works)
        
        if not work_refs_map:
            return []
        
        completed_works = []
        
        try:
            # ray.wait()ë¡œ ì™„ë£Œëœ ì‘ì—… í™•ì¸
            object_refs = list(work_refs_map.values())
            ready_refs, _ = ray.wait(
                object_refs,
                num_returns=len(object_refs),
                timeout=timeout
            )
            
            # ì—­ë°©í–¥ ë§¤í•‘ ìƒì„± (ObjectRef -> work_id)
            ref_to_work_id = {ref: wid for wid, ref in work_refs_map.items()}
            
            # readyëœ ì‘ì—…ë“¤ ì²˜ë¦¬
            for ready_ref in ready_refs:
                # ObjectRefì— í•´ë‹¹í•˜ëŠ” work_id ì°¾ê¸°
                completed_work_id = ref_to_work_id.get(ready_ref)
                
                if completed_work_id is None:
                    # ì´ë¡ ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, Ray ë‚´ë¶€ ë²„ê·¸ë¥¼ ëŒ€ë¹„í•œ ë°©ì–´ ì½”ë“œ
                    logger.error(
                        f"ğŸ› UNEXPECTED: Could not find work_id for completed ref: {ready_ref}. "
                        f"This should never happen. Possible Ray bug."
                    )
                    try:
                        ray.get(ready_ref, timeout=0)
                    except Exception as cleanup_error:
                        logger.debug(f"Error cleaning up orphaned ref: {cleanup_error}")
                    continue
                
                try:
                    # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                    result = ray.get(ready_ref)
                    
                    logger.info("=" * 80)
                    logger.info(f"âœ… Work {completed_work_id} execution completed: {result.get('status')}")
                    logger.info("=" * 80)

                    # Work ìƒíƒœ ì—…ë°ì´íŠ¸ (ì½œë°± ì‹¤íŒ¨ì™€ ë¬´ê´€í•˜ê²Œ ìœ ì§€ë˜ì–´ì•¼ í•¨)
                    self._on_execution_complete(completed_work_id, result)
                    
                    completed_works.append((completed_work_id, result))
                    
                    # ì½œë°± í˜¸ì¶œ (ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ Work ìƒíƒœì— ì˜í–¥ ì—†ë„ë¡ ë³„ë„ ì²˜ë¦¬)
                    with self.lock:
                        callback = self.work_callbacks.get(completed_work_id)
                    
                    if callback:
                        try:
                            callback(completed_work_id, result)
                        except Exception as callback_error:
                            logger.error(
                                f"âš ï¸ Callback error for work {completed_work_id} "
                                f"(Work itself succeeded): {callback_error}",
                                exc_info=True
                            )
                    
                except Exception as e:
                    logger.error(f"âŒ Error processing work {completed_work_id}: {str(e)}", exc_info=True)
                    self._update_work_status_safe(completed_work_id, WorkStatus.FAILED)
                    error_result = {
                        'work_id': completed_work_id,
                        'status': JobExecutionStatus.FAILED.to_str(),
                        'error': str(e)
                    }
                    
                    completed_works.append((completed_work_id, error_result))
                    
                    # ì½œë°± í˜¸ì¶œ (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                    with self.lock:
                        callback = self.work_callbacks.get(completed_work_id)
                    
                    if callback:
                        try:
                            callback(completed_work_id, error_result)
                        except Exception as callback_error:
                            logger.error(
                                f"âš ï¸ Callback error for work {completed_work_id}: {callback_error}",
                                exc_info=True
                            )
                
                finally:
                    # pending_worksì—ì„œ ì œê±°
                    with self.lock:
                        self.pending_works.pop(completed_work_id, None)
                        self.work_callbacks.pop(completed_work_id, None)
            
        except Exception as e:
            logger.error(f"ğŸ’¥ Critical error in ray.wait(): {str(e)}", exc_info=True)
            # ray.wait() ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°, ëª¨ë“  pending worksë¥¼ FAILEDë¡œ ì²˜ë¦¬
            for work_id in list(work_refs_map.keys()):
                logger.error(f"âŒ Marking work {work_id} as FAILED due to ray.wait() error")
                self._update_work_status_safe(work_id, WorkStatus.FAILED)
                
                error_result = {
                    'work_id': work_id,
                    'status': JobExecutionStatus.FAILED.to_str(),
                    'error': f'Ray wait error: {str(e)}'
                }
                
                # ì½œë°± í˜¸ì¶œ
                with self.lock:
                    callback = self.work_callbacks.get(work_id)
                
                if callback:
                    try:
                        callback(work_id, error_result)
                    except Exception as callback_error:
                        logger.error(f"Error calling callback for work {work_id}: {callback_error}")
                
                completed_works.append((work_id, error_result))
                
                # pending_worksì—ì„œ ì œê±°
                with self.lock:
                    self.pending_works.pop(work_id, None)
                    self.work_callbacks.pop(work_id, None)
        
        return completed_works
    
    def log_execution_result(self, work_id: int, result: Dict[str, Any]) -> None:
        """
        ì‹¤í–‰ ê²°ê³¼ ë¡œê¹…
        
        Args:
            work_id: Work ID
            result: ì‹¤í–‰ ê²°ê³¼
        """
        logger.info("ğŸ“Š Execution Result:")
        logger.info(f"  Work ID: {work_id}")
        logger.info(f"  Status: {result.get('status')}")
        logger.info(f"  Duration: {result.get('total_duration', 0):.2f}s")
        logger.info(f"  Data items: {len(result.get('data_items', []))}")
        logger.info(f"  Errors: {result.get('error_count', 0)}")
        
        # Primary Task ê²°ê³¼
        if result.get('primary_task'):
            primary = result['primary_task']
            logger.info(f"  Primary Task: {primary['status']} ({primary.get('duration', 0):.2f}s)")
        
        # ê° ë°ì´í„° ì•„ì´í…œ ì²˜ë¦¬ ê²°ê³¼
        for item_idx, item_result in enumerate(result.get('data_items', []), 1):
            logger.info(f"  Item {item_idx}: {item_result['status']} ({item_result.get('duration', 0):.2f}s)")
            for task_info in item_result.get('tasks', []):
                status_icon = "âœ“" if task_info['status'] == 'success' else "âœ—"
                logger.info(
                    f"    {status_icon} {task_info['task_name']}: "
                    f"{task_info['status']} ({task_info.get('duration', 0):.2f}s)"
                )
    
    def get_executor_status(self) -> Dict[str, Any]:
        """
        Executor ìƒíƒœ ë°˜í™˜
        
        Returns:
            Executor ìƒíƒœ ì •ë³´
        """
        with self.lock:
            pending_count = len(self.pending_works)
        
        return {
            'num_executors': self.num_executors,
            'current_executor_idx': self.current_executor_idx,
            'pending_works_count': pending_count
        }

